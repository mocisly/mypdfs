\documentclass{article}
\usepackage{preamble}
\usepackage{env}

% available environments:
% theorem: thm
% definition: defn
% proof: pf
% corollary: crll
% lemma: lm
% question: qu
% solution: soln
% example: xmp
% exercise: exr
%
% options: title=<title>   {all}
%          source=<source> {pf, qu, soln, xmp, exr}
%               Note: if content is taken directly from the main resource,
%                     cite the main resource as ``Primary source material"


% define these variables!
\def\coursecode{MAT257Y5}
\def\coursename{} % use \relax for non-course stuff
\def\studytype{} % 1: Personal Self-Study Notes / 2: Course Lecture Notes / 3: Revised Notes / 4: Exercise Solution Sheet
\def\author{\me}
\def\createdate{}
\def\updatedate{\today}
\def\source{} % name, ed. of textbook, or `Class Lectures` for class notes
\def\sourceauthor{} % for class notes, put lecturer
\def\leftmark{Big List Problems} % set text in header; should only be necessary in assignments etc.
\pagenumbering{arabic} % force revert numbering to default; should only be necessary in assignments etc.

\makeatletter
% settings for toc alignment
%
% Configuration
% -------------
% Horizonal alignment in \numberline:
%   l: left-aligned
%   c: centered
%   r: right-aligned
% \nl@align@: Default setting
% \nl@align@<levelname>: Setting for specific level

\def\nl@align@{l}% default
\def\nl@align@section{r}

\makeatother

\begin{document}

\setcounter{subsection}{1}

% q1
\begin{qu}
    Let $ A_{1}, A_{2}, A_{3}, \dots $ be a sequence of countable sets.
    Prove that $ \bigcup_{i \geq 1} A_{i} $ is countable.
\end{qu}

\begin{soln}[title=Using Axiom of (Countable) Choice]
    Let $ A = \bigcup_{i \geq 1} A_{i} $. 
    Recall that $ \bb{N} \times \bb{N} $ is a countable set.
    We will construct a bijection $ \vphi : A \rightarrow \bb{N} \times \bb{N} $. \npgh

    First, note that we can assume that each $ A_{i} $ is non-empty. \vsp
    Indeed, if there exists an empty set $ A_{k} $, we can consider $ A = \bigcup_{i \geq 1} A_{i} \setminus A_{k} $.
    If there are infinitely many empty sets, then their union is also the empty set, and removing these sets from $ A $ equates to removing the empty set. \vsp
    %
    Next, recall that for any $ A_{i} $, since it is countable, then there exists an injection $ f_{i} : A_{i} \rightarrow \bb{N} $.
    We define the collection of such injective functions as:
    \begin{equation*}
        \cl{F} = \set{f_{i} : A_{i} \rightarrow \bb{N} \mid f_{i} \textrm{ injective}, i \in \bb{N}}
    \end{equation*}
    \textbf{Note that the existence of this set requires the Axiom of (Countable) Choice.} \\
    Now, we're ready to construct our bijection. \npgh

    Let $ a \in A $. We define $ \vphi(a) $ as:
    \begin{equation*}
        \vphi(a) = (n, f_{i}(a))
    \end{equation*}
    where $ n $ is given by the smallest $ A_{n} $ such that $ a \in A_{n} $, 
    $ f_{n} : A_{n} \rightarrow \bb{N} $ is the injection in $ \cl{F} $, 
    and $ f_{n}(a) \in \bb{N} $ is the natural number uniquely mapped to by $ a $.
    It suffices to show that $ \vphi $ is injective. \vsp
    %
    Indeed, suppose $ a, b \in A $ and $ \vphi(a) = \vphi(b) $.
    Then, we see that:
    \begin{align*}
        & \vphi(a) = \vphi(b) \\
        \implies \ & (n_{a}, f_{n_{a}}(a)) = (n_{b}, f_{n_{b}}(b)) \\
        \implies \ & n_{a} = n_{b} , f_{n_{a}}(a) = f_{n_{b}}(b)
    \end{align*}
    Therefore, since $ n_{a} = n_{b} $, we'll denote $ n := n_{a} $. Then:
    \begin{align*}
        & f_{n_{a}}(a) = f_{n}(a) = f_{n}(b) = f_{n_{b}}(b) \\
        \implies \ & a = b
    \end{align*}
    where the final implication comes from the fact that each $ f_{i} $ is injective. \vsp
    %
    Thus, we have proved that $ \vphi $ is injective, therefore proving that $ A $ is countable as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q2
\begin{qu}
    Let $ X, Y, Z $ be three vector spaces.
    Prove that $ L^{2}(X, Y ; Z) $ is isomorphic to $ L(X, L(Y, Z)) $.
\end{qu}

\begin{soln}
    The idea of this proof is to show that for any bilinear function in $ L^{2}(X, Y; Z) $, by fixing $ x \in X $, we can treat it as a linear function in $ L(Y, Z) $ by considering the $ x- $slice.
    To this end, we will construct a bijection $ \Phi : L(X, L(Y, Z)) \rightarrow L^{2}(X, Y ; Z) $. \vsp
    %
    Suppose $ \psi \in L(X, L(Y, Z)) $, that is $ \psi : X \rightarrow L(Y, Z) $ is linear.
    Intuitively, $ \psi $ takes an element $ x \in X $ and ``binds" it to a linear map $ T_{x}: Y \rightarrow Z $.
    Indeed, $ \psi(x) = T_{x} $ for some $ T_{x} \in L(Y, Z) $. \vsp
    %
    Now, if we consider any $ S(x, y) \in L^{2}(X, Y ; Z) $, we can consider the $ x_{0} $-slice of $ S $ for some fixed $ x_{0} \in X $. 
    Since $ S $ is bilinear, then $ S(x_{0}, y) $ is indeed linear.
    Therefore, we can write $ S(x_{0}, y) = T_{x_{0}}(y) $ for some $ T \in L(Y, Z) $. \vsp
    %
    Based on this, we will define $ \Phi $ such that:
    \begin{equation*}
        \Phi(\psi) = S_{\psi}(x, y) \textrm{ s.t. } S_{\psi}(x, y) = \psi(x)(y) = T_{x}(y)
    \end{equation*}
    It remains to show that $ \Phi $ is linear, injective, and surjective. \vsp
    %
    Let $ \psi_{1}, \psi_{2} \in L(X, L(Y, Z)) $, and $ c \in \bb{F} $ a non-zero constant from the base field. \\
    Then, we have that:
    \begin{align*}
        & \Phi(c\psi_{1} + \psi_{2}) \\
        = \ & (c\psi_{1} + \psi_{2})(x)(y) \\
        = \ & c\psi_{1}(x)(y) + \psi_{2}(x)(y) \\
        = \ & c\Phi(\psi_{1}) + \Phi(\psi_{2})
    \end{align*}
    So $ \Phi $ is linear. Next, we want to show injectivity:
    \begin{equation*}
        \Phi(\psi_{1}) = \Phi(\psi_{2}) \implies \ \psi_{1}(x)(y) = \psi_{2}(x)(y)
    \end{equation*}
    Since $ \psi_{1} $ and $ \psi_{2} $ agree for all points $ x \in X, y \in Y $,
    then we can indeed conclude that $ \psi_{1} = \psi_{2} $.
    Lastly, we want to show surjectivity.
    Consider any $ S(x, y) \in L^{2}(X, Y; Z) $.
    Since $ S $ is bilinear, then any $ x_{0} $-slice $ S(x_{0}, y) $ is a linear map.
    Denote this slice as $ S(x_{0}, y) = T_{x_{0}}(y) $. 
    For all $ x_{0} \in X $, define $ \vphi : X \rightarrow L(Y, Z) $ as:
    \begin{equation*}
        x_{0} \rightarrow T_{x_{0}}(y)
    \end{equation*}
    It remains to show that $ \vphi $ is linear. Indeed, we see that:
    \begin{align*}
        & \vphi(ca + b) \\
        = \ & T_{ca + b}(y) \\
        = \ & S(ca + b, y) \\
        = \ & cS(a, y) + S(b, y) \\
        = \ & cT_{a}(y) + T_{b}(y) \\
        = \ & c\vphi(a) + \vphi(b)
    \end{align*}
    Finally, we note that $ \Phi(\phi) = T_{x}(y) = S(x, y) $, so $ \Phi $ is indeed surjective.
    Thus, we have shown that $ \Phi $ is linear and bijective,
    therefore it is a linear isomorphism as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q3
\begin{qu}
    Let $ I = (a, b) $ and $ J = (c, d) $ be two open intervals on the real line.
    Let $ f: I \rightarrow J $ be an increasing function such that $ f(I) $ is dense in $ J $.
    Prove that $ f $ is continuous.
\end{qu}

\begin{soln}
    Suppose for the sake of contradiction that $ f $ is not continuous.
    That is, there exists some point $ c \in I $ such that $ f $ is not continuous at $ c $. \vsp
    %
    Since $ f $ is monotonic, then its left and right limits must exist:
    \begin{equation*}
        a = \lim_{x \rightarrow c^{-}}f(x) < f(c) < \lim_{x \rightarrow c^{+}}f(x) = b
    \end{equation*}
    Note that one of the above inequalities may be an equality, but not both. \vsp
    %
    Define the following interval:
    \begin{equation*}
        K = \begin{cases} (a, f(c)) & a \neq f(c) \\ (a, b) & a = f(c) \end{cases}
    \end{equation*}
    Notice that $ K \subseteq J $, and that $ f(c) \notin K $.
    However, since $ f(c) \notin K $, then we have that:
    \begin{equation*}
        K \cap f(I) = \varnothing
    \end{equation*}
    However, this is a contradiction, as $ f(I) $ is dense in $ J $.
    Therefore, there cannot be any point $ c \in I $ at which $ f $ is discontinuous at $ c $,
    which means that $ f $ must be continuous.
\end{soln}

\newpage
\stepcounter{subsection}

% q4.1
\begin{qu}
    Prove that there exists an infinitely differentiable function
    $ \alpha: \bb{R} \rightarrow \bb{R} $ such that $ \alpha(t) = 0 $ for all
    $ t \leq 0 $, and $ \alpha(t) > 0 $ for all $ t > 0 $.
\end{qu}

\begin{soln}
    We define a function, and prove it is infinitely differentiable on all $ \bb{R} $.
    Consider the function
    \begin{equation*}
        \alpha(t) = \begin{cases} 0 & t \leq 0 \\ e^{-1/t} & t > 0 \end{cases}
    \end{equation*}
    Clearly, $ \alpha $ is infinitely differentiable for $ t < 0 $.
    We show the cases $ t > 0 $ and $ t = 0 $ separately. \npgh

    Suppose $ t > 0 $. We will show by induction that $ \alpha^{(n)}(t) $ is of the form
    \begin{equation*}
        \alpha^{(n)}(t) = \frac{d^{n}}{dt^{n}} e^{-1/t} = e^{-1/x} P_{2n}
        \left( \frac{1}{t} \right)
    \end{equation*}
    where $ \alpha^{(n)}(t) $ is the $ n $-th derivative of $ \alpha $ at $ t $,
    and $ P_{2n}(\frac{1}{t}) $ is a polynomial of degree $ 2n $ over the variable $ 1/t $. \vsp
    %
    Base case: Let $ n = 1 $.
    Then, $ a^{(1)}(t) = \dfrac{d}{dt} e^{-1/t} $ is given by:
    \begin{equation*}
        \frac{d}{dt} e^{-1/t} = \frac{e^{-1/t}}{t^{2}} = e^{-1/t} \left( \frac{1}{t} \right)^{2}
    \end{equation*}
    Induction step: Suppose our claim is true for some $ n = k $.
    We show that it holds for $ n = k +1 $.
    For the sake of readability, denote $ y = 1/t $. Indeed, we see that:
    \begin{align*}
        \frac{d^{k+1}}{dt^{k+1}} \alpha(t) = & \frac{d}{dt} \alpha^{(k)}(t) \\
                    = & \frac{d}{dt} e^{-y}P_{2k}(y) \\
                    = & \frac{d}{dt} e^{-y}\sum_{i = 0}^{2k} {a_{i}y^{i}} \\
                    = & y^{2} e^{-y} \sum_{i=0}^{2k} {a_{i}y^{i}}
                    + e^{-y} \sum_{i = 0}^{2k-1} {(i+1)a_{i+1}y^{i}} \\
                    = & e^{-y} \left( \sum_{i=0}^{2k} {a_{i}y^{i+2}}
                    + \sum_{i=0}^{2k-1} {(i+1)a_{i+1}y^{i+2}} \right) \\
                    = & e^{-y} \left( P_{2k+2}(y) + P_{2k+1}(y) \right) \\
                    = & e^{-1/t} P_{2(k+1)} \left( \frac{1}{t} \right)
    \end{align*}
    Therefore, by induction, our claim is proven.
    Since we know what the $ n $-th derivative looks like for $ \alpha(t) $ with $ t > 0 $
    for any arbitary $ n \in \bb{N} $, we can conclude that $ \alpha(t) $ is infinitely
    differentiable on all $ t > 0 $ as needed. \npgh
    
    It remains to show that $ \alpha(t) $ is infinitely differentiable at $ t = 0 $.
    To do this, we will again proceed by induction, with the claim that
    the $ n $-th derivative of $ \alpha(0) = 0 $. \vsp
    %
    Base case: Let $ n = 1 $.
    Then, we have that
    \begin{align*}
        \frac{d}{dt} \alpha(0) & = \lim_{h\rightarrow 0} \frac{\alpha(0 + h) - \alpha(0)}{h} \\
                               & = \lim_{h \rightarrow 0} \frac{\alpha(h)}{h}
    \end{align*}
    We examine each side of the limit.
    Clearly, the left-sided limit is equal to 0, so we 
    denote $ y = 1/h $ and examine the right-sided limit. We see that:
    \begin{align*}
        & \lim_{ h \rightarrow 0^{+}} \frac{e^{-1/h}}{h} \\
        = \ & \lim_{y \rightarrow \infty} \frac{y}{e^{y}} \\
        = \ & \lim_{y \rightarrow \infty} \frac{1}{e^{y}} \\
        = \ & 0
    \end{align*}
    Where we use L'Hopital's Rule in the third line.
    Then, since the left and right-sided limits are equal,
    the original limit must exist and be equal to 0.
    Therefore, $ \alpha'(0) = 0 $ as needed. \vsp
    %
    Induction step: Suppose $ \alpha^{(k)}(0) = 0 $.
    We show that $ \alpha^{(k+1)}(0) = 0 $. Indeed:
    \begin{align*}
        \frac{d^{n}}{dt^{n}} \alpha(0) & = \lim_{h \rightarrow 0} 
        \frac{\alpha^{(k)}(h + 0) - \alpha^{(k)}(0)}{h} \\
                    & = \lim_{h \rightarrow 0} \frac{\alpha^{(k)}(h)}{h}
    \end{align*}
    Once again, we examine the sided limits, letting $ y = 1/h $ as needed.
    Again, the left-sided limit is trivially 0. Examining the right-sided limit, we see that:
    \begin{align*}
        & \lim_{h \rightarrow 0^{+}} \frac{a^{(k)}(h)}{h} \\
        = \ & \lim_{h \rightarrow 0^{+}} \frac{e^{-y}P_{2k}(y)}{h} \\
        = \ & \lim_{y \rightarrow \infty} \frac{P_{2k+1}(y)}{e^{y}} \\
        = \ & \lim_{y \rightarrow \infty} \frac{P_{2k}(y)}{e^{y}} \\
            & \vdots \\
        = \ & \lim_{y \rightarrow \infty} \frac{cy}{e^{y}} \\
        = \ & \lim_{y \rightarrow \infty} \frac{c}{e^{y}} \\
        = \ & 0
    \end{align*}
    where $ c $ is some constant, and we apply L'Hopital's Rule $ 2k + 1 $ times.
    Since each sided limit is equal to 0, then the original limit must equal 0,
    which means that $ \alpha^{(k+1)}(0) = 0 $ as needed. \npgh

    We have now shown that $ \dfrac{d^{n}}{dt^{n}} \alpha(t) $ exists for all $ t \in \bb{R} $,
    so $ \alpha $ is infinitely differentiable as needed.
\end{soln}

% q4.2
\begin{qu}
    Prove that there exists an infinitely differentiable function
    $ \beta: \bb{R} \rightarrow \bb{R} $ such that $ \beta(t) = 1 $ for all $ t \geq 1 $,
    and $ \beta(t) = 0 $ for all $ t \leq 0 $.
\end{qu}

\begin{soln}
    Consider the function defined by:
    \begin{equation*}
        \beta(t) = \frac{\alpha(t)}{\alpha(t) + \alpha(1-t)}
    \end{equation*}
    where $ \alpha(t) $ is the function defined in part 1. \vsp
    %
    It is easy to see that $ \beta(t) $ is infinitely differentiable,
    since $ \beta(t) $ consists of copies of $ \alpha(t) $ along with basic operations
    which preserve the infinite differentiability. \vsp
    %
    Clearly, if $ t \geq 1 $, then $ \alpha(1 - t) = 0 $. Thus, for any $ t \geq 1 $, we see that
    \begin{equation*}
        \beta(t) = \frac{\alpha(t)}{\alpha(t)} = 1
    \end{equation*}
    Similarly, if $ t \leq 0 $, then $ \alpha(1 - t) > 0 $, so:
    \begin{equation*}
        \beta(t) = \frac{0}{\alpha(1 - t)} = 0
    \end{equation*}
\end{soln}

% q4.3
\begin{qu}
    Prove that there exists an infinitely differentiable function
    $ \vphi: \bb{R} \rightarrow \bb{R} $ such that $ \vphi(t) = 1 $ for all $ t \in [2, 3] $,
    and $ \vphi(t) = 0 $ for all $ t \in \bb{R} \setminus (1, 4) $.
\end{qu}

\begin{soln}
    Consider the function given by:
    \begin{equation*}
        \vphi(t) = \beta(t - 1)\beta(4 - t)
    \end{equation*}
    Clearly, $ \vphi $ is infinitely differentiable. \vsp
    %
    Consider $ t \leq 1 $. Then, $ \beta(t - 1) = 0 $, so $ \vphi(t) = 0 $.
    Similarly, if $ t \geq 4 $, then $ \beta(4 - t) = 0 $, so $ \vphi(t) = 0 $. \vsp
    %
    Now, suppose $ t \in [2, 3] $. Then, $ t - 1 \in [1, 2] \geq 0 $ and $ 4 - t \in [1, 2] \geq 0 $.
    So, we see that $ \vphi(t) = 1 $ as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q5
\begin{qu}
    Let $ S \subseteq \bb{R}^{n} $. Consider the following statements:
    \begin{itemize}
        \item $ S $ is a bounded subset of $ (\bb{R}^{n}, \norm{\cdot}_{1}) $.
        \item $ S $ is a bounded subset of $ (\bb{R}^{n}, \norm{\cdot}_{2}) $.
        \item $ S $ is a bounded subset of $ (\bb{R}^{n}, \norm{\cdot}_{\textrm{max}}) $.
    \end{itemize}
    Prove the six implications between these statements.
\end{qu}
Although it suffices to prove three statements in a cyclical manner,
we will prove all six. Note that each section of the proof will get its own block for
the sake of readability.

\begin{soln}[title=1 implies 2]
    Suppose $ S $ is a bounded subset of $ (\bb{R}^{n}, \norm{\cdot}_{1}) $.
    Let $ x \in S $. Then:
    \begin{equation*}
        \norm{x}_{2} = \sqrt{\sum_{i=1}^{n} {x_{i}^{2}} } \leq \sum_{i=1}^{n} {\sqrt{x_{i}^{2}}} 
        = \sum_{i=1}^{n} {\abs{x_{i}}} = \norm{x}_{1}
    \end{equation*}
    Clearly, we see that if $ S $ is bounded on $ \norm{\cdot}_{1} $,
    then $ S $ is bounded on $ \norm{\cdot}_{2} $.
\end{soln}

\begin{soln}[title=2 implies Max]
    Suppose $ S $ is a bounded subset of $ (\bb{R}^{n}, \norm{\cdot}_{2}) $.
    Let $ x \in S $. Then, for some $ M \geq 0 $, it follows that:
    \begin{equation*}
        \norm{x}_{\textrm{max}}^{2} \leq \sqrt{\sum_{i=1}^{n} {x_{i}^{2}} } = \norm{x}_{2} \leq M
    \end{equation*}
    Se we see that if $ S $ is bounded on $ \norm{\cdot}_{2} $, then
    $ S $ is bounded on $ \norm{\cdot}_{\textrm{max}} $.
\end{soln}

\begin{soln}[title=Max implies 1]
    Suppose $ S $ is a bounded subset of $ (\bb{R}^{n}, \norm{\cdot}_{\textrm{max}}) $.
    Let $ x \in S $. Then, for some $ M \geq 0 $, we have that:
    \begin{equation*}
        \norm{x}_{1} = \sum_{i=1}^{n} {\abs{x_{i}}} \leq \sum_{i=1}^{n} {\norm{x}_{\textrm{max}}}
        = nM
    \end{equation*}
    Clearly, if $ S $ is bounded on $ \norm{\cdot}_{\textrm{max}} $,
    then $ S $ is bounded on $ \norm{\cdot}_{1} $.
\end{soln}

\newpage
\begin{soln}[title=1 implies Max]
    Suppose $ S $ is a bounded subset of $ (\bb{R}^{n}, \norm{\cdot}_{1}) $.
    Let $ x \in S $. Then:
    \begin{equation*}
        \norm{x}_{\textrm{max}} \leq \sum_{i=1}^{n} {\abs{x_{i}}} = \norm{x}_{1}
    \end{equation*}
    Clearly, if $ S $ is bounded on $ \norm{\cdot}_{1} $, then
    $ S $ is bounded on $ \norm{\cdot}_{\textrm{max}} $.
\end{soln}

\begin{soln}[title=Max implies 2]
    Suppose $ S $ is a bounded subset of $ (\bb{R}^{n}, \norm{\cdot}_{2}) $.
    Let $ x \in S $. Then, since $ \norm{x}_{\textrm{max}} \leq M $ for some $ M \geq 0 $,
    it follows that:
    \begin{equation*}
        \norm{x}_{2} = \sqrt{\sum_{i=1}^{n} {x_{i}^{2}} } \leq nM^{2}
    \end{equation*} since each $ x_{i}^{2} $ is necessarily upper bounded by
    $ \norm{x}_{\textrm{max}}^{2} \leq M^{2} $.
    Thus, if $ S $ is bounded on $ \norm{\cdot}_{2} $, then
    $ S $ is bounded on $ \norm{\cdot}_{\textrm{max}} $.
\end{soln}

\begin{soln}[title=2 implies 1]
    Suppose $ S $ is a bounded subset of $ (\bb{R}^{n}, \norm{\cdot}_{2}) $.
    Let $ x \in S $. Then, for some $ M \geq 0 $, we have that:
    \begin{equation*}
        \norm{x}_{1} = \sum_{i=1}^{n} {\abs{x_{i}}} = \sum_{i=1}^{n} {1\abs{x_{i}}}
        = \abs{\abs{x} \cdot \hat{1}} \leq \sqrt{n} \sqrt{\sum_{i=1}^{n} {\abs{x_{i}}^{2}} } 
        = \sqrt{n} \ \norm{x}_{2} \leq \sqrt{n} M
    \end{equation*}
    The vector $ \hat{1} $ is used to denote the vector with all entries as $ 1 $,
    and the first inequality comes from the Cauchy-Schwarz inequality. \vsp
    %
    Thus, we see that if $ S $ is bounded on $ \norm{x}_{2} $, then $ S $
    is bounded on $ \norm{x}_{1} $ as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q6.1
\begin{qu}
    Prove that $ B(X, Y) $ is a linear subspace of $ L(X, Y) $.
\end{qu}

\begin{soln}
    Let $ S, T \in B(X, Y) $ be bounded linear mappings and $ c \in \bb{F} $ a constant
    from the base field.
    We want to show that $ S + cT $ is a bounded linear mapping. \vsp
    %
    Indeed, for all $ x \in X $, we see that:
    \begin{align*}
         & \norm{(S + cT)(x)}_{Y} \\
        = \ & \norm{S(x) + cT(x)}_{Y} \\
        \leq \ & \norm{S(x)}_{Y} + \norm{cT(x)}_{Y} \\
        = \ & \norm{S(x)}_{Y} + \abs{c} \norm{T(x)}_{Y} \\
        \leq \ & M_{S}\norm{x}_{X} + \abs{c}M_{T}\norm{x}_{X} \\
        = \ & (M_{S} + \abs{c}M_{T})\norm{x}_{X}
    \end{align*}
    So $ S + cT $ is bounded by $ M = M_{S} + \abs{c}M_{T} $, therefore $ S + cT \in B(X, Y) $.
\end{soln}

% q6.2
\begin{qu}
    Prove that $ \norm{\cdot}_{\textrm{op}} $ is a norm on $ B(X, Y) $.
\end{qu}

\begin{soln}
    We prove the three properties of a norm. \npgh

    Since $ \norm{T}_{\trm{op}} $ is the supremum of $ \norm{T(x)}_{Y} $ for certain $ x $,
    then clearly $ \norm{\cdot}_{\trm{op}} $ is positive-definite.
    Additionally, it is trivial to see that if $ T $ is the null-map,
    then $ \norm{T}_{\trm{op}} = 0 $.
    It remains to show that $ \norm{T}_{\trm{op}} \implies T $ is the null-map. \vsp
    %
    Suppose $ \norm{T}_{\trm{op}} = 0 $.
    Then, for all $ x \in X $ such that $ \norm{x}_{X} \leq 1 $, we see that:
    \begin{align*}
        & \norm{T}_{\trm{op}} = 0 \\
        \implies \ & \sup \set{\norm{T(x)}_{Y}} = 0 \\
        \implies \ & \norm{T(x)}_{Y} = 0 \\
        \implies \ & T(x) = 0
    \end{align*}
    So, we see that for all $ x $ such that $ \norm{x}_{X} \leq 1, T(x) = 0 $.
    Now, consider the vector $ \dfrac{x}{\norm{x}_{X}} $ for any non-zero $ x \in X $.
    We see that:
    \begin{align*}
        & T \left( \frac{x}{\norm{x}_{X}} \right) = 0 \\
        \implies \ & \frac{1}{\norm{x}_{X}} T(x) = 0 \\
        \implies \ & T(x) = 0
    \end{align*}
    So indeed, we see that $ T $ is the null-map on $ X $ as needed. \npgh

    Next, we prove homogeneity. We see that:
    \begin{align*}
        & \norm{cT}_{\trm{op}} \\
        = \ & \sup \set{\norm{cT(x)}_{Y} : x \in X, \norm{x}_{X} \leq 1} \\
        = \ & \sup \set{\abs{c}\norm{T(x)}_{Y} : x \in X, \norm{x}_{X} \leq 1} \\
        = \ & \abs{c} \ \sup \set{\norm{T(x)}_{Y} : x \in X, \norm{x}_{X} \leq 1} \\
        = \ & \abs{c} \ \norm{T}_{\trm{op}}
    \end{align*}
    as needed. \npgh

    Lastly, we prove triangle inequality. We see that:
    \begin{align*}
        & \norm{S + T}_{\trm{op}} \\
        = \ & \sup \set{\norm{(S + T)(x)}_{Y} : x \in X, \norm{x}_{X} \leq 1} \\
        = \ & \sup \set{\norm{S(x) + T(x)}_{Y} : x \in X, \norm{x}_{X} \leq 1} \\
        \leq \ & \sup \set{\norm{S(x)}_{Y} + \norm{T(x)}_{Y} : x \in X, \norm{x}_{X} \leq 1} \\
        \leq \ & \sup \set{\norm{S(x)}_{Y} : x \in X, \norm{x}_{X} \leq 1} +
        \sup \set{\norm{T(x)}_{Y} : x \in X, \norm{x}_{X} \leq 1} \\
        = \ & \norm{S}_{\trm{op}} + \norm{T}_{\trm{op}}
    \end{align*}
    as needed. \npgh

    We have proven all three properties of a norm, therefore $ \norm{\cdot}_{\trm{op}} $
    is indeed a norm on $ B(X, Y) $.
\end{soln}

% q6.3
\begin{qu}
    Let $ T: \bb{R}^{2} \rightarrow \bb{R}^{2} $ be the linear mapping given by
    $ T(x, y) = (x + y, x) $. Find, with proof, the exact value of $ \norm{T}_{\trm{op}} $.
\end{qu}

For the week 2 homework, I did not submit part 3 of Question 6 from the big list.
This resubmission is submitted in accordance to the resubmit request and policy.

Portfolio draft edit: the below solution still has a potentially erroneous step which needs to be
fixed.

\begin{soln}
    Since $ T $ is a linear map, we can write it as a matrix:
    \begin{equation*}
        T =
        \begin{bmatrix}
            1 & 1 \\ 1 & 0
        \end{bmatrix}
    \end{equation*}
    We take the characteristic polynomial:
    \begin{equation*}
        C_{T}(x) = \det(xI - T) =
        \det\begin{bmatrix}
            x-1 & -1 \\ -1 & x
        \end{bmatrix}
        = x(x - 1) - 1 = x^{2} - x - 1
    \end{equation*}
    We know from the Fibonacci sequence that the roots of this polynomial are
    $ \vphi $ and $ \overline{\vphi} $, where $ \vphi $ is the golden ratio.
    Therefore, we know that:
    \begin{equation*}
        T = PAP^{-1} \quad A = \begin{bmatrix}
            \vphi & 0 \\ 0 & \overline{\vphi}
        \end{bmatrix}
    \end{equation*}
    for some invertible matrix $ P $.
    Since $ P $ is a change of basis and therefore preserves the inner product, we can
    consider only $ A $. \vsp
    %
    Clearly, taking $ x = (1, 0) $, we get that $ \norm{T}_{\trm{op}} \geq \vphi $.
    Suppose that $ \norm{T}_{\trm{op}} > \vphi $. Then, there must exist some $ x = (x_{1}, x_{2}) $
    such that $ \norm{x} \leq 1 $ and $ \norm{T(x)}_{2} > \vphi $. Then:
    \begin{gather*}
        \vphi < \sqrt{\vphi^{2}x_{1}^{2} + \overline{\vphi}^{2}x_{2}^{2}}
        = \sqrt{(\vphi + 1)x_{1}^{2} + \frac{1}{\vphi + 1}x_{2}^{2}} \\
        \implies \vphi^{2} = \vphi + 1 < (\vphi + 1)x_{1}^{2} + \frac{x_{2}^{2}}{\vphi + 1} \\
        \implies 1 < x_{1}^{2} + \frac{x_{2}^{2}}{(\vphi + 1)^{2}} < x_{1}^{2} + x_{2}^{2}
    \end{gather*}
    But this is a contradiction, since $ \norm{x} \leq 1 $.
    Therefore, it must be that $ \norm{T}_{\trm{op}} = \vphi $.
\end{soln}

% q6.4
\begin{qu}
    Find, with proof, an example of an unbounded linear operator.
\end{qu}

\begin{soln}
    Let $ X = Y = P(\bb{R}) $ be the vector spaces of all polynomials with real coefficients,
    where the coefficients $ \set{a_{i}}_{i\geq1} $ is a sequence in $ \ell^{ \infty} $.
    Define the norm on $ X, Y $ as $ \norm{v} = \sup \set{\abs{a_{1}}, \abs{a_{2}}, \dots} $,
    where each $ a_{i} $ is the coefficient of $ x^{i} $.
    Note that  $ \norm{\cdot} $ is equivalent to the sup norm on $ \ell^{ \infty} $. \vsp
    %
    Consider the linear mapping $ T: X \rightarrow Y $ given by $ T(v) = \frac{d}{dx} v $.
    We show that $ T $ is unbounded. \npgh
    
    Indeed, suppose that $ M \geq 0 $.
    Denote by $ \oline{M} = \lceil M \rceil $.
    \begin{equation*}
        v = \oline{M}x^{\oline{M} + 1}
    \end{equation*}
    Clearly, $ \norm{v} = \oline{M} $, and
    $ \norm{T(v)} = \oline{M}(\oline{M} + 1) > \oline{M}^{2}= M \norm{v} $.
    Note that if $ M = 0 $, then it suffices to choose any non-constant polynomial as $ v $. \vsp
    %
    Thus, we have shown that for any $ M \geq 0 $,
    there exists $ v \in X $ such that $ \norm{T(v)} > M \norm{v} $,
    so $ T $ is unbounded as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q7
\begin{qu}
    Let $ S \subseteq C[0, 1] $. Consider the following statements:
    \begin{enumerate}
        \item $ S $ is an open subset of $ (C[0, 1], \norm{\cdot}_{1}) $.
        \item $ S $ is an open subset of $ (C[0, 1], \norm{\cdot}_{\infty}) $.
    \end{enumerate}
    Determine if the first implies the second or vice-versa.
\end{qu}

\begin{soln}
    First, we show that $ (2) \implies (1) $.
    Suppose $ S $ is an open subset of $ (C[0, 1], \norm{\cdot}_{\infty}) $.
    Let $ f \in S $ be any function.
    Then, we have an open ball $ B_{\infty}(f, r) $ in the sup norm.
    Let $ g \in B_{\infty}(f, r) $. Then we have that $ \norm{f - g}_{\infty} < r $.
    We see that:
    \begin{equation*}
        \norm{f - g}_{1} = \int_{0}^{1} \abs{(f - g)(x)} dx < \int_{0}^{1} \abs{r} dx = r
    \end{equation*}
    Therefore, it follows that $ B_{1}(f, r) \subseteq B_{\infty}(f, r) $.
    Since this is true for all $ f $,
    then we have that $ S $ is open in $ (C[0, 1], \norm{\cdot}_{1}) $ as needed. \npgh

    Next, we show that $ (1) \cnot\implies (2) $. Note that the sequence $ (x_{n})_{n \geq 1} $
    given by $ x_{n} = x^{n} $ converges to $ 0 $, since:
    \begin{equation*}
        \int_{0}^{1} x^{n} dx = \frac{1}{n + 1}
    \end{equation*}
    So we can conclude that for all $ \ep > 0 $, there exists some $ N \geq 0 $ such that
    for all $ n \geq N $, we have that $ \norm{x^{n}}_{1} < \ep $.
    This tells us that we indeed have $ x^{n} \rightarrow 0 $ under the 1-norm. \vsp
    %
    Now, suppose that there exists an open ball $ B_{\infty}(0, r) $ which is contained in
    an open ball $ B_{1}(0, \ep) $. In particular, $ r < \ep $. \vsp
    %
    Then, we must have that there exists some $ N_{r} \geq 0 $ such that for all $ n \geq N_{r} $,
    we have that $ \norm{x^{n}}_{\infty} < r $. But this is a contradiction, since we clearly
    have that $ \norm{x^{n}}_{\infty} = 1 $, and we can choose $ \ep $ to be arbitrarily small.
    Therefore, we cannot inscribe an open ball $ B_{\infty}(0, \ep) $ within
    a ball $ B_{1}(0, \delta) $. This is sufficient to show that $ (1) \cnot \implies (2) $
    as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q8
\begin{qu}
    Let $ X $ be any set.
    The \textbf{diagonal} of $ X \times X $ is:
    \begin{equation*}
        \Delta = \set{(x, x) : x \in X}
    \end{equation*}
    Prove that if $ (X, d) $ is a metric space, then $ \Delta $ is a closed subset of
    $ X \times X $ (with respect to the product metric).
\end{qu}

\begin{soln}
    Consider a sequence $ ((x_{i}, x_{i}))_{i \geq 1} $ in $ \Delta $.
    Suppose that $ (x_{i}, x_{i}) \rightarrow (a, b) $ for some $ (a, b) \in X \times X $.
    We show that $ a = b $. \vsp
    %
    Since $ (x_{i}, x_{i}) \rightarrow (a, b) $, we must have that $ x_{i} \rightarrow a $
    and $ x_{i} \rightarrow b $. In particular, for all $ \ep > 0 $, there exist $ N_{1}, N_{2} $
    such that for all $ i \geq N = \max(N_{1}, N_{2}) $, we have that:
    \begin{equation*}
        d(x_{i}, a) < \frac{1}{2}\ep \quad , \quad d(x_{i}, b) < \frac{1}{2}\ep
    \end{equation*}
    Therefore, we see that:
    \begin{equation*}
        d(a, b) \leq d(a, x_{i}) + d(b, x_{i}) < \frac{1}{2}\ep + \frac{1}{2}\ep = \ep
    \end{equation*}
    Since this is true for all $ \ep > 0 $, then it must be true that $ a = b $ as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q9.1
\begin{qu}
   Let $C^\infty[0,1]$ denote the set of infinitely differentiably functions
   $f:[0,1]\rightarrow \bb{R}$. Prove that $C^\infty[0,1]$ is \textit{not} a closed subset of
   $(C[0,1],\norm{\cdot}_{\infty})$.
\end{qu}

\begin{soln}
    Consider the sequence $ (f_{n})_{n \geq 1} $ given by:
    \begin{equation*}
        f_{n} = \sqrt{ \left( x - \frac{1}{2} \right)^{2} + \frac{1}{n}}
    \end{equation*}
    Clearly, each $ f_{n} $ is infinitely differentiable on $ [0, 1] $. However, we see that:
    \begin{equation*}
        f_{n} \rightarrow f , \quad f(x) = \sqrt{ \left( x - \frac{1}{2} \right)^{2}}
        = \abs{x - \frac{1}{2}}
    \end{equation*}
    So $ f_{n} $ does not converge to a differentiable function, so $ f \notin C^{\infty}[0, 1] $.
    Therefore, $ C^{\infty}[0, 1] $ is not a closed subset as needed.
\end{soln}

% q9.2
\begin{qu}
    Let $ C $ be the set of convergent sequence of real numbers.
    Prove that $ C $ is a closed subset of $ (\ell^{\infty}, \norm{\cdot}_{\infty}) $.
\end{qu}

\begin{soln}
    Let $ (c_{n}) $ be a sequence of convergent sequences in $ C $.
    Suppose that $ c_{n} \rightarrow s $ for some $ s \in \ell^{\infty} $.
    Then, we must have that:
    \begin{itemize}
        \item For all $ \ep > 0 $, there exists $ N_{1} \geq 0 $ such that for all
            $ i, j \geq N_{1} $, we have that $ \norm{c_{ni} - c_{nj}} < \frac{1}{3}\ep $.
        \item For all $ \ep > 0 $, there exists $ N_{2} \geq 0 $ such that
            for all $ i \geq N_{2} $, we have that $ \norm{c_{ij} - s_{j}} < \frac{1}{3}\ep $.
    \end{itemize}
    Let $ N = \max(N_{1}, N_{2}) $. Then, for all $ i, n, m \geq N $, we see that:
    \begin{align*}
        & \norm{s_{n} - s_{m}} \\
        = \ & \norm{s_{n} + c_{in} - c_{in} + c_{im} - c_{im} - s_{m}} \\
        \leq \ & \norm{s_{n} - c_{in}} + \norm{c_{in} - c_{im}} + \norm{c_{im} - s_{m}} \\
        < \ & \frac{1}{3}\ep + \frac{1}{3}\ep + \frac{1}{3}\ep \\
        = \ & \ep
    \end{align*}
    Therefore, our sequence $ s $ is Cauchy. Since $ \bb{R} $ is complete,
    this means that $ s $ must converge, and so is in $ C $.
    Thus, since $ C $ contains its limit points, then it is a closed subset as needed.
\end{soln}
\newpage

% q9.3
\begin{qu}
    part 3
\end{qu}

\newpage
\stepcounter{subsection}

% q10.1
\begin{qu}
    Prove that every bounded sequence in $ \bb{R}^{d}, (\norm{\cdot}_{2}) $ has
    a convergent subsequence.
\end{qu}

From week 3 submission - the previous submission was somewhat unclear and also had a few issues
with the structure of the proof. The below is a revision to make the proof more clear.

\begin{soln}
    We prove by induction on $ d $. \vsp
    %
    Base case: Suppose $ \bb{R}^{d} = \bb{R} $. This has already been proven. \vsp
    %
    Induction step: Suppose the claim holds for some \textcolor{Emerald}{natural number $ k $}.
    We want to show it holds for $ k + 1 $. \vsp
    %
    Suppose $ (x_{i})_{i \geq 1} $ is a \textcolor{Emerald}{bounded} sequence in $ \bb{R}^{k+1} $.
    \textcolor{Emerald}{
    Then for each $ x_{i} $, we have that:
    \begin{equation*}
        \norm{x_{i}} = \sqrt{\sum_{j=1}^{k+1} x_{ij}} \leq M
    \end{equation*}
    Note that:
    \begin{equation*}
        \sqrt{\sum_{j=1}^{k} x_{ij}} \leq \sqrt{\sum_{j=1}^{k+1}x_{ij}} = \norm{x_{i}} \leq M
    \end{equation*}
    That is, each term in the sequence is bounded in the first $ k $ coordinates.
    } \vsp
    %
    By our I.H., we know that there is a subsequence $ (x_{i_{j}})_{j \geq 1} $ such that
    the first $ k $ coordinates converge. \textcolor{Emerald}{For readability, we will re-index
    this sequence as $ (x_{n})_{n\geq1} $, using $ n $ for Nigel.} \vsp
    %
    Since $ (x_{n}) $ is a subsequence of a bounded sequence, then it is also bounded.
    Therefore, \textcolor{Emerald}{by our base case,}
    there is a subsequence $ (x_{n_{m}}) $ such that the $ (k+1) $-th coordinate
    also converges. \vsp
    %
    Since each coordinate of the subsequence $ (x_{n_{m}}) $ converges, then
    the sequence converges pointwise to some $ x \in \bb{R}^{k+1} $.
    Therefore, by induction, every bounded sequence must have a convergent subsequence as needed.
\end{soln}

% q10.2
\begin{qu}
    Give an example of a normed vector space $(X, \norm{\cdot})$ containing a
    bounded sequence $(x_n)$ which has no convergent subsequences.
\end{qu}

\begin{soln}
    Consider the normed vector space $ (C[0, 1], \norm{\cdot}_{\infty}) $, and the sequence
    given by $ (f_{n})_{n \geq 1} $ as $ f_{n} = x^{n} $. \vsp
    %
    Clearly, this is a bounded sequence as for each $ x^{n} $, we have $ \norm{x^{n}} = 1 $.
    So the sequence is bounded, but it has no convergent subsequences as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q11.1
\begin{qu}
    Suppose $ (X, d) $ is a complete metric space and $ f : X \rightarrow X $ a contraction
    mapping. Prove that $ f $ has a unique fixed point.
\end{qu}

This is a resubmission of the Banach Fixed Point Theorem (Big List 11).
We tidy the proof for part a) according to the feedback, add the (forgotten) proof
of the uniqueness of the fixed point, and add an example for part b).

\begin{soln}
    Let $ x_{0} \in X $ be any point, and define a sequence $ (x_{n})_{n \geq 1} $ given as:
    \begin{equation*}
        x_{n} = f(x_{n-1})
    \end{equation*}
    Note that for each $ n $-th term:
    \begin{equation*}
        d(x_{n+1}, x_{n}) \leq Md(x_{n}, x_{n-1}) \leq M^{2}d(x_{n-1}, x_{n-2}) \leq \cdots
        \leq M^{n}d(x_{1}, x_{0})
    \end{equation*}
    Furthermore, by the triangle inequality, we have:
    \begin{align*}
        d(x_{n+m}, x_{n}) & \leq d(x_{n+m}, x_{n+m-1}) + d(x_{n+m-1}, x_{n}) \\
                          & \leq d(x_{n+m}, x_{n+m-1}) + d(x_{n+m-1}, x_{n+m-2})
                          + d(x_{n+m-2}, x_{n}) \\
                          & \ \: \vdots \\
                          & \leq \sum_{j=n}^{n+m-1} d(x_{j+1}, x_{j}) \\
                          & \leq \sum_{j=n}^{n+m-1} M^{j} d(x_{1}, x_{0}) \\
                          & \textcolor{Emerald}{
                            \leq M^{n}d(x_{1}, x_{0})\sum_{j=0}^{m-1} M^{j}
                          } \\
                          & \textcolor{Emerald}{
                            \leq M^{n}d(x_{1}, x_{0})\sum_{j=0}^{\infty}M^{j}
                          } \vsp
                          & \textcolor{Emerald}{
                            = \frac{M^{n}d(x_{1}, x_{0})}{1 - M}
                          } \vsp
                          & < \textcolor{Emerald}{M^{n}}d(x_{1}, x_{0})
    \end{align*}
    Since $ M \in (0, 1) $, then $ \lim_{n \rightarrow \infty} M^{n} = 0 $.
    Therefore as $ m \rightarrow \infty $, we get that $ d(x_{n+m}, x_{n}) \rightarrow 0 $.
    Since this holds for any $ n $-th term, then we can conclude that the sequence is Cauchy,
    and therefore $ x_{n} \rightarrow x $ for some $ x \in X $.
    Then, for any $ x_{n} $:
    \begin{equation*}
        d(f(x), x) \leq d(f(x), f(x_{n})) + d(f(x_{n}), x) \leq Md(x, x_{n}) + d(x_{n+1}, x)
    \end{equation*}
    Since $ x_{n} \rightarrow x $, then $ d(x_{n}, x) \rightarrow 0 $. Therefore, we have that:
    \begin{equation*}
        d(f(x), x) = 0 \implies f(x) = x
    \end{equation*}
    So we have that $ x $ is a fixed point of $ f $ as needed.
\end{soln}

\begin{soln}[title=Uniqueness]
    Next, we show that the fixed point is unique.
    For this, suppose $ M \in (0, 1) $ and $ f $ is a contraction mapping. \vsp
    %
    Suppose $ x_{1} $ and $ x_{2} $ are distinct fixed points of $ f $. Then:
    \begin{gather*}
        d(f(x_{1}), f(x_{2})) < d(x_{1}, x_{2}) \qquad
        f(x_{1}) = x_{1} \qquad
        f(x_{2}) = x_{2} \vsp
        \implies \ d(f(x_{1}), f(x_{2})) = d(x_{1}, x_{2}) < d(x_{1}, x_{2})
    \end{gather*}
    Clearly, this is a contradiction, so there cannot be multiple fixed points.
\end{soln}

% q11.2
\begin{qu}
    Give an example of a normed vector space and a contraction mapping such that $ f $ does
    \textbf{not} have a fixed point.
\end{qu}

\begin{soln}
    Consider $ (C[0,1], \norm{\cdot}_{1}) $.
    Let $ B = B \left( 0, \dfrac{1}{2} \right) $, and define:
    \begin{equation*}
        \kappa: B \rightarrow B \qquad \kappa(f) = f^{2} + \frac{1}{4}
    \end{equation*}
    To see that it is a contraction mapping, let $ f, g \in B $ such that $ f \neq g $. Then:
    \begin{align*}
        & \int_{0}^{1} \abs{f^{2}(x) - g^{2}(x)} \ dx \\
        = \ & \int_{0}^{1} \abs{f(x) - g(x)} \abs{f(x) + g(x)} \ dx \\
        = \ & \norm{f - g} \norm{f + g} \\
        < \ & \norm{f - g}
    \end{align*}
    Note that the last inequality follows from the fact that $ f, g \in B $. \vsp
    %
    Next, to see that it has no fixed points, notice that any fixed point would satisfy:
    \begin{equation*}
        f = f^{2} + \frac{1}{4}
        \ \implies \ f^{2} - f + \frac{1}{4} = 0
        \ \implies \ \left( f - \frac{1}{2} \right)^{2} = 0
        \ \implies \ f = \frac{1}{2}
    \end{equation*}
    However, the constant function $ f = \dfrac{1}{2} \notin B $.
    Therefore, $ \kappa $ is a contraction mapping with no fixed points as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q12.1
\begin{qu}
    Let $ I \subseteq \bb{R} $.
    Prove that $ I $ is an interval if and only if $ I $ is connected.
\end{qu}

\begin{soln}
    First, we show that if $ I $ is an interval, then it is connected. \npgh
    
    Suppose $ I $ is disconnected. Then, $ I = A \sqcup B $ for some
    non-empty disjoint open sets $ A, B \subseteq \bb{R} $.
    Let $ a, b \in I $ such that $ a \in A, b \in B $. WLOG, assume $ a < b $. \vsp
    %
    Since $ A $ is open, then there exists some maximal $ r > 0 $ such that
    $ [a, a + r) \subseteq A $.
    Consider the sequence $ (a_{n})_{n\geq1} $ in $ A $ given by:
    \begin{equation*}
        a_{i} = a + r - \frac{1}{n}
    \end{equation*}
    Clearly, this sequence converges to $ a_{0} = a + r \notin [a, a+r) $.
    Since $ r $ is maximal, then this implies that $ a_{0} \in B $. \vsp
    %
    Since $ B $ is open, then there exists some $ \ep_{0} $ such that
    $ B(a_{0}, \ep_{0}) \subseteq B $.
    Since $ (a_{n}) $ converges to $ a_{0} $, then for all $ \ep > 0 $,
    there exists some $ M \geq 0 $ such that:
    \begin{equation*}
        n \geq M \implies \abs{a_{0} - a_{n}} < \ep
    \end{equation*}
    In particular, there exists some $ M_{0} $ such that for all $ n \geq M_{0} $,
    we have that $ \abs{a_{0} - a_{n}} < \ep_{0} $.
    But this implies that each $ a_{n} \in B(a_{0}, \ep_{0}) \subseteq B $, so $ a_{n} \in B $.
    This is contradiction, so we must have that $ I $ is an interval as needed. \npgh

    Next, we show that if $ I $ is not an interval, then it is disconnected. \npgh

    Suppose $ I $ is not an interval.
    Then, there exist $ a, b \in I $ such that $ [a, b] \nsubseteq I $.
    We must also have that $ (a, b) \nsubseteq I $; indeed, if this is not the case,
    then it follows that either $ a $ or $ b $ is not in $ I $, which is a contradiction.
    Therefore, we fix $ c \in (a, b) $ such that $ c \notin I $. Consider:
    \begin{equation*}
        A_{c} = (-\infty, c) \qquad B_{c} = (c, \infty)
    \end{equation*}
    Note that $ A_{c} \cap B_{c} = \varnothing $, so they are disjoint. Consider the sets:
    \begin{equation*}
        A = I \cap A_{c} \qquad B = I \cap B_{c}
    \end{equation*}
    Clearly, $ A $ and $ B $ are disjoint, and $ I = A \cup B $.
    Since $ A $ and $ B $ are open sets with respect to the subspace topology,
    then $ I $ is disconnected as needed.
\end{soln}

% q12.2
\begin{qu}
    Let $ X, Y $ be two metric spaces, and $ f : X \rightarrow Y $ a continuous function.
    Prove that if $ C $ is a connected subset of $ X $,
    then $ f(C) $ is a connected subset of $ Y $.
\end{qu}

\begin{soln}
    We prove the contrapositive: if $ f(C) $ is disconnected, then $ C $ is disconnected. \vsp
    %
    Suppose $ f(C) = A \cup B $ for some non-empty, disjoint open sets $ A, B \subseteq Y $.
    Since $ f $ is continuous, then $ f^{-1}(A) $ and $ f^{-1}(B) $ are open.
    Additionally, since $ A $ and $ B $ are disjoint, then their pre-images must also be disjoint.
    We show that $ f^{-1}(A) \cup f^{-1}(B) = C $. \vsp
    %
    Suppose otherwise; that is, there exists $ c \in C \setminus (f^{-1}(A) \cup f^{-1}(B)) $.
    Since $ c \in C $, then:
    \begin{equation*}
        f(c) \in C \implies f(c) \in A \cup B
    \end{equation*}
    WLOG, suppose $ f(c) \in A $.
    But this implies that $ c \in f^{-1}(A) $, which is a contradiction.
    Therefore, we must have that $ C $ is the union of two disjoint, open subsets of $ X $,
    and is thus disconnected as needed.
\end{soln}

% q12.3
\begin{qu}
    Prove that the Intermediate Value Theorem (in $ \bb{R} $) follows from parts 1 and 2, and thus
    part 2 is a generalization of IVT.
\end{qu}

\begin{soln}
    Let $ I \subseteq \bb{R} $ be an interval, and $ f : I \rightarrow \bb{R} $ be continuous.
    Fix $ a, b \in f(I) $ such that $ a < b $, and fix some $ a < y < b $. \vsp
    %
    Since $ I $ is an interval, then it is connected.
    Since $ f $ is continuous, then $ f(I) $ is also connected. Therefore:
    \begin{equation*}
        [a, b] \subseteq f(I) \implies y \in f(I)
    \end{equation*}
    Since $ y \in f(I) $, then there exists some $ x \in I $ such that $ f(x) = y $ as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q13.1
\begin{qu}
    q13 pt1
\end{qu}

% q13.2
\begin{qu}
    q13 pt2
\end{qu}

\newpage
\stepcounter{subsection}

% q14
\begin{qu}
    q14
\end{qu}

\newpage
\stepcounter{subsection}

% q15
\begin{qu}
    q15
\end{qu}

\newpage
\stepcounter{subsection}

% q16
\begin{qu}
    q16
\end{qu}

\newpage
\stepcounter{subsection}

% q17
\begin{qu}[title=Lebesgue Number Lemma]
    Let $ X $ be a clustering metric space. Prove that every open cover has a Lebesgue number.
\end{qu}

\begin{soln}
    Let $ \cl{U} = \set{U_{i}}_{i \in I} $ be an open cover of $ X $.
    Suppose for the sake of contradiction that $ \cl{U} $ does not have a Lebesgue number.
    In particular, it must be true that for all $ \delta > 0 $, there exists $ x_{\delta} \in X $
    such that $ B(x_{\delta}, \delta) \nsubseteq U_{i} $ for all $ U_{i} \in \cl{U} $. \vsp
    %
    Define a sequence $ (x_{n})_{n \geq 1} $ where each $ x_{n} $ is such that:
    \begin{equation*}
        B\left( x_{n}, \frac{1}{n} \right) \nsubseteq U_{i}, \quad U_{i} \in \cl{U}
    \end{equation*}
    Since $ X $ is clustering, then WLOG, the sequence converges to some $ x \in X $.
    Then, there must exist some $ U_{x} \in \cl{U} $ such that $ x \in U_{x} $. It follows that:
    \begin{equation*}
        B(x, \ep) \subseteq U_{x}, \quad \trm{for some } \ep > 0
    \end{equation*}
    Since $ x_{n} \rightarrow x $, then there exist $ M_{1}, M_{2} \in \bb{N} $ such that
    for all $ m_{1} \geq M_{1}, m_{2} \geq M_{2} $, we have that:
    \begin{equation*}
        x_{m_{1}} \in B(x, \ep) \qquad d(x, x_{m_{2}}) < \ep - \frac{1}{m_{2}}
    \end{equation*}
    Let $ M = \max(M_{1}, M_{2}) $. Then, for all $ m \geq M $, we see that:
    \begin{equation*}
        p \in B \left( x_{m}, \frac{1}{m} \right)
        \ \implies \ d(x, p) \leq d(x, x_{m}) + d(x_{m}, p) < \ep - \frac{1}{m} + \frac{1}{m} = \ep
    \end{equation*}
    This implies that $ B \left( x_{m}, \frac{1}{m} \right) \subseteq B(x, \ep) $, and so:
    \begin{equation*}
        B \left( x_{m}, \frac{1}{m} \right) \subseteq B(x, \ep) \subseteq U_{x}
        \ \implies \ B \left( x_{m}, \frac{1}{m} \right) \subseteq U_{x}
    \end{equation*}
    But this is a contradiction, so $ \cl{U} $ must indeed have a Lebesgue number as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q18.1
\begin{qu}
    q18 pt 1
\end{qu}

% q18.2
\begin{qu}
    q18 pt2
\end{qu}

\newpage
\stepcounter{subsection}

% q19.1
\begin{qu}
    Let $ U \subseteq \bb{R}^{n} $ be a bounded open set. For all $ n \in \bb{N} $, define:
    \begin{equation*}
        K_{n} = \set{p \in U : \norm{p - x} \geq \frac{1}{n} \trm{ for all } x \in \partial U}
    \end{equation*}
    Show that this is a compact exhaustion of $ U $.
\end{qu}

\begin{soln}
    First, we show that each $ K_{n} $ is compact.
    Clearly each $ K_{n} \subseteq U $, so we show that $ K_{n} $ is closed.
    Consider $ K_{n}^{c} $. We can write it as:
    \begin{align*}
        K_{n}^{c} = \ & U^{c} \cup \set{p \in U: \norm{p - x} <
        \frac{1}{n} \trm{ for some } x \in \partial U} \vsp
        = \ & U^{c} \cup \left( \bigcup_{x \in \partial U}
        B \left( x, \frac{1}{n} \right) \right) \vsp
        = \ & (U^{c} \setminus \partial U) \cup \left( \bigcup_{x \in \partial U}
        B \left( x, \frac{1}{n} \right) \right)
    \end{align*}
    We see that each of these sets are open, and so the entire union is an open set.
    Therefore, $ K_{n}^{c} $ is open, so $ K_{n} $ is closed, and therefore compact as needed. \vsp
    %
    Next, we show that $ U = \bigcup_{n \geq 1}K_{n} $.
    Indeed, since $ U $ is open, then for any $ x \in U $, there exists some $ \ep > 0 $ such that
    $ B(x, \ep) \subseteq U $. It clearly follows that there exists a sufficiently large $ n $
    such that:
    \begin{equation*}
        B \left( x, \frac{1}{n} \right) \subseteq B(x, \ep) \subseteq U
    \end{equation*}
    It thus follows that $ x \in K_{n} $ for some $ n $. \vsp
    %
    Lastly, we show that for all $ n $, we have that $ K_{n} \subseteq K_{n+1}^{\circ} $.
    Suppose $ p_{0} \in K_{n} $ for some fixed $ n $. \vsp
    Let $ \delta = \dfrac{1}{n(n+1)} $. We show that $ B(p_{0}, \delta) \subseteq K_{n+1} $.
    Indeed, let $ p \in B(p_{0}, \delta) $, and fix $ x_{0} \in \partial U $ where
    $ \norm{p - x_{0}} \leq \norm{p - x} $ for all $ x \in \partial U $. Then:
    \begin{align*}
        \norm{p_{0} - x_{0}} \leq \ & \norm{p_{0} - p} + \norm{p - x_{0}} \vsp
        \implies \ \norm{p - x_{0}} \geq \ & \norm{p_{0} - x_{0}} - \norm{p_{0} - p} \\
        > \ & \frac{1}{n} - \delta \\
        = \ & \frac{1}{n} - \frac{1}{n(n+1)} \\
        = \ & \frac{n}{n(n+1)} \\
        = \ & \frac{1}{n+1}
    \end{align*}
    It follows that $ \norm{p - x} \geq \dfrac{1}{n+1} $ for all $ p \in B(p_{0}, \delta) $ and
    $ x \in \partial U $, so $ B(p_{0}, \delta) \subseteq K_{n+1} $ as needed. \vsp
    Since this holds for all $ p_{0} \in K_{n} $,
    then we have that $ K_{n} \subseteq K_{n+1}^{\circ} $ as needed.
\end{soln}

% q19.2
\begin{qu}
    Now, show that \textit{every} open subset of $ \bb{R}^{n} $ has a compact exhaustion.
\end{qu}

\begin{soln}
    Let $ U $ be an open subset of $ \bb{R}^{n} $. Define the set $ U_{n} $ as:
    \begin{equation*}
        U_{n} = U \cap B(0, n)
    \end{equation*}
    Note that for all $ n $, we have that $ U_{n} \subseteq U_{n+1} $.
    Define $ K_{n} $ as follows:
    \begin{equation*}
        K_{n} = \bigcup_{k=1}^{n} K_{k, n}
    \end{equation*}
    where $ K_{k, n} $ is the set defined in part a), replacing $ U $ with $ U_{k} $.
    Clearly, $ K_{n} $ is compact as it is the finite union of compact sets.
    Next, we show that $ U = \bigcup_{n \geq 1} K_{n} $. \vsp
    %
    Let $ x \in U $. Then, fix any $ k \in \bb{N} $ such that $ k > \norm{x} $.
    Then, it follows that:
    \begin{equation*}
        k > \norm{x} \implies x \in U_{k}
    \end{equation*}
    By the same argument in part a), there exists some $ n $ such that $ x \in K_{k, n} $,
    so $ x \in K_{n} $ as needed. \vsp
    %
    Lastly, we show that $ K_{n} \subseteq K_{n+1}^{\circ} $.
    Note that by part a), we have that $ K_{k, n} \subseteq K_{k, n+1}^{\circ} $.
    Recall that:
    \begin{gather*}
        K_{n} = \bigcup_{k=1}^{n} K_{k, n} \vsp
        K_{n+1}^{\circ} = \left( \bigcup_{k=1}^{n+1}K_{k,n+1} \right)^{\circ}
        = \bigcup_{k=1}^{n+1}K_{k,n+1}^{\circ}
    \end{gather*}
    Since $ (A \cup B)^{\circ} = A^{\circ} \cup B^{\circ} $ for any
    $ A, B \subseteq \bb{R}^{n} $, then the result follows.
\end{soln}

\newpage
\stepcounter{subsection}

% q20
\begin{qu}
    Let $ x $ be a sequence in $ (\ell^{\infty}, \norm{\cdot}_{\infty}) $.
    Prove that $ D_{x} $ is compact if and only if $ x_{n} \rightarrow 0 $.
\end{qu}

\begin{soln}
    First, suppose that $ x_{n} \rightarrow 0 $.
    We show that $ D_{x} $ is complete and totally bounded. \vsp
    %
    It is easy to see that $ D_{x} $ is complete, as it is a closed subset of a complete space.
    To see that it is totally bounded, let $ \ep > 0 $, and set $ s = \norm{x} $. \vsp
    %
    Now, consider the real interval $ (-s, s) $. Clearly, it is totally bounded, and so there
    exists a finite set of points such that:
    \begin{equation*}
        C = \set{c_{1}, c_{2}, \dots, c_{m}} \qquad
        (-s, s) \subseteq \bigcup_{c \in C} B(c, \ep)
    \end{equation*}
    Consider the subset $ \cl{C} \subseteq \ell^{\infty} $ defined as:
    \begin{equation*}
        \cl{S} = \set{k_{i} : k_{i} = (c_{i}, c_{i}, \dots), i \in \bb{N}, i \leq m}
    \end{equation*}
    Then, for any $ y \in D_{x} $, since $ \norm{y} \leq \norm{x} $, it follows that:
    \begin{equation*}
        y \in B(k_{i}, \ep) \trm{ for some } k_{i} \in S
    \end{equation*}
    Therefore, $ D_{x} $ is totally bounded, and therefore is compact as needed. \npgh

    Now, suppose that $ x_{n} \cnot\rightarrow 0 $.
    We show that $ D_{x} $ is not sequentially compact. \vsp
    %
    Since $ x_{n} \cnot\rightarrow 0 $, then there exists some $ \ep > 0 $ such that:
    \begin{equation*}
        \abs{x_{n_{i}}} \geq \ep \trm{ for infinitely many } n_{i}
    \end{equation*}
    Define a sequence $ s_{j} \in D_{x} $ as:
    \begin{equation*}
        s_{j_{k}} = \begin{cases} \ep & j = k \\ 0 & j \neq k \end{cases}
    \end{equation*}
    Then, we see that $ (s_{j}) $ is a sequence in $ D_{x} $ which does not have
    a convergent subsequence, and so $ D_{x} $ is not sequentially compact as needed.
\end{soln}

\newpage
\stepcounter{subsection}

% q21.1
\begin{qu}
    q21 pt1
\end{qu}

% q21.2
\begin{qu}
    q21 pt2
\end{qu}

\newpage
\stepcounter{subsection}

% q22
\begin{qu}
    Let $ c_{0} $ denote the set of sequences converging to zero.
    Prove that $ c_{0}^{*} \equiv \ell^{1} $.
\end{qu}

\newpage
\stepcounter{subsection}

% q23.1
\begin{qu}
    Find an explicit formula for $ \Phi : S^{2} \setminus \set{(0, 0, 1)} \rightarrow \bb{R}^{2} $,
    given as the stereographic projection of the unit $ 2 $-sphere onto $ \bb{R}^{2} $.
\end{qu}

\begin{soln}
    First, consider the problem reduced by one dimension.
    We want to find $ \Phi(P) $ satisfying:

    \centering
    \scalebox{.95}{\incfig{stereoproject2d}}
    \flushleft

    Here, we can use properties of similar triangles to derive that:
    \begin{equation*}
        \frac{\Phi(P)}{1} = \frac{x}{1-y}
    \end{equation*}
    So $ \Phi(x, y) = \dfrac{x}{1-y} $. To extend this up a dimension to our original problem,
    we see that:
    
    \centering
    \scalebox{.95}{\incfig{stereoproject3d}}
    Still unsure how these diagrams are getting to school... Bad joke?
    \flushleft
    
    In other words, we can project the point $ P $ onto the $ xz $-plane and $ yz $-plane, then
    projecting those points onto the $ xy $-plane in order to get the $ x $-component and
    $ y $-component respectively of $ \Phi(P) $. As a formula, we get:
    \begin{equation*}
        \Phi(x, y, z) = \left( \frac{x}{1-z}, \frac{y}{1-z} \right)
    \end{equation*}
\end{soln}

% q23.2
\begin{qu}
    Deduce that $ \Phi $ is continuous.
\end{qu}

\begin{soln}
    id rather not tbh
\end{soln}

% q23.3
\begin{qu}
    Given $ p = (s, t) \in \bb{R}^{2} $, find an explicit formula for $ \Phi^{-1}(p) $.
\end{qu}

\begin{soln}
    We want to solve for $ x, y, $ and $ z $ such that:
    \begin{equation*}
        s = \frac{x}{1-z} \qquad t = \frac{y}{1-z} \qquad x^{2} + y^{2} + z^{2} = 1
    \end{equation*}
    First, notice that:
    \begin{equation*}
        x = (1-z)s \qquad y = (1-z)t
    \end{equation*}
    Then, rearranging the equation of the unit sphere, we get that:
    \begin{align*}
        & x^{2} + y^{2} = 1 - z^{2} \\
        \implies \ & x^{2} + y^{2} = (1-z)(1+z) \\
        \implies \ & (1-z)^{2}s^{2} + (1-z)^{2}t^{2} = (1-z)(1+z) \\
        \implies \ & s^{2} + t^{2} = \frac{1+z}{1-z}
    \end{align*}
    Denoting $ N = s^{2} + t^{2} $, and rearranging, we get that:
    \begin{equation*}
        1 + z = N(1 - z) \ \implies \ z = \frac{N - 1}{N + 1}
    \end{equation*}
    Finally, substituting this value of $ z $ into our other equations, we get that:
    \begin{equation*}
        x = (1 - z)s \ \implies \ x = \frac{2s}{N + 1} \qquad
        y = (1 - z)t \ \implies \ y = \frac{2t}{N + 1}
    \end{equation*}
    This gives us the final formula for the inverse as:
    \begin{equation*}
        \Phi^{-1}(s, t) = \left( \frac{2s}{s^{2} + t^{2} + 1},
        \frac{2t}{s^{2} + t^{2} + 1}, \frac{s^{2} + t^{2} - 1}{s^{2} + t^{2} + 1}\right)
    \end{equation*}
\end{soln}

% q23.4
\begin{qu}
    Deduce that $ \Phi $ is a homeomorphism.
\end{qu}

\begin{soln}
    again, id rather not
\end{soln}

\newpage
\stepcounter{subsection}

% q24
\begin{qu}
    Let $ X $ be a normed vector space. Prove the following are equivalent:
    \begin{enumerate}
        \item $ X $ is finite dimensional.
        \item The unit ball $ \oline{B}(0, 1) $ is compact.
        \item $ X $ is locally compact; that is, every point is contained in some open set with
            a compact closure.
   \end{enumerate}
\end{qu}

\begin{soln}
    $ (1 \implies 2) $
    Since $ X $ is finite-dimensional, then $ X $ is homeomorphic to $ \bb{R}^{n} $.
    This immediately tells us the closed unit ball is compact. \npgh

    $ (2 \implies 3) $
    Indeed, if the closed unit ball is compact, then for any point $ p $, we can define
    the isometry:
    \begin{equation*}
        \Delta(x) = x + p
    \end{equation*}
    This is trivially an isometry, as:
    \begin{equation*}
        \norm{\Delta(x) - \Delta(y)} = \norm{x + p - y - p} = \norm{x - y}
    \end{equation*}
    Therefore, we can take the unit ball centered at $ p $.
    This is isometric to the open unit ball, whose closure is compact, and so the closure of
    the open unit ball centered at $ p $ is also compact as needed. \npgh

    $ (3 \implies 1) $
    e
\end{soln}

\newpage
\stepcounter{subsection}

% q25
\begin{qu}
    q25
\end{qu}

\newpage
\stepcounter{subsection}

% q26.1
\begin{qu}
    q26 pt1
\end{qu}

% q26.2
\begin{qu}
    q26 pt2
\end{qu}

% q26.3
\begin{qu}
    q26 pt3
\end{qu}

\newpage
\stepcounter{subsection}

% q27.1
\begin{qu}
    q27 pt1
\end{qu}

% q27.2
\begin{qu}
    q27 pt2
\end{qu}

% q27.3
\begin{qu}
    q27 pt3
\end{qu}

\newpage
\stepcounter{subsection}

% q28.1
\begin{qu}
    q28 pt1
\end{qu}

% q28.2a
\begin{qu}
    q28 pt2a
\end{qu}

% q28.2b
\begin{qu}
    q28 pt2b
\end{qu}

% q28.2c
\begin{qu}
    q28 pt2c
\end{qu}

% q28.2d
\begin{qu}
    q28 pt2d
\end{qu}

\newpage
\stepcounter{subsection}

% q29.1
\begin{qu}
    q29 pt1
\end{qu}

% q29.2
\begin{qu}
    q29 pt2
\end{qu}

% q29.3
\begin{qu}
    q29 pt3
\end{qu}

\newpage
\stepcounter{subsection}

% q30
\begin{qu}
    q30
\end{qu}

\newpage
\stepcounter{subsection}

% q31.1
\begin{qu}
    q31 pt1
\end{qu}

% q31.2
\begin{qu}
    q31 pt2
\end{qu}

% q31.3
\begin{qu}
    q31 pt3
\end{qu}

\end{document}

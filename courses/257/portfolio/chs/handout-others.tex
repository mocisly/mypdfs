\subsection{Written by others}

\subsubsection{Week 0 | Exercise 0.4}

The following is Exercise 0.4 from Week 0.
This was solved together with Ali and Justin.

\begin{exr}[num=0.4]
    Let $V,W$ be two vector spaces. Prove that $\dim(V)\leq \dim(W)$ if and only
    if there exists an injective linear mapping $T:V\rightarrow W$.
\end{exr}

\begin{pf}[source=Justin]
    We will first prove the backwards implication; assume there exists an
    injective linear map $T:V\rightarrow W$. Let $\beta$ be a basis of $V$.
    Consider $T(\beta)$, which is simply the image of $T$ under $\beta$.
    By assumption of $T$'s injectivity,
    $\abs{\beta}=\abs{T(\beta)}$. Also, as $\spans (T(\beta))$ is a subspace of
    $W$, $\dim (\spans T(\beta))\leq \dim (W)$. Thus it follows 
    \[
    \dim (V)=\abs{\beta}=\dim (\spans T(\beta))\leq \dim (W)
    \]

    It remains to prove the forward implication. Assume that
    $\dim(V)\leq\dim(W)$. Let $\beta, \gamma$ be bases of $V,W$ respectively.
    Then $\abs{\beta}= \dim(V)$ and $\abs{\gamma}=\dim (W)$.
    There exists some $\delta\subseteq \gamma$ such that $\abs{\delta}
    =\abs{\beta}$. Since $\delta$ is a subset of $\gamma$ and $\gamma$ is
    linearly independent, $\delta$ too is linearly independent. \vsp
    %
    Additionally, we have that $\spans(\delta)$ is a subspace of $W$ with basis
    $\delta$. As $\abs{\delta}=\abs{\beta}$, $\dim(V)=\dim(\spans \delta)$.
    Thus there exists an isomorphism $T:V\rightarrow \spans{\delta}$.
    Now, define a function $S:V\rightarrow W$ given by 
    $S(v)=T(v)$ for all $v\in V$. As constructed, $S$ is linear and injective.
\end{pf}

\newpage
\subsubsection{Week 1 | Exercise 1.37}

The following is Exercise 1.37 from Week 1.
This was solved together with Kabir.

\begin{exr}[num=1.37]
    Formulate and prove the following statement.
    \textit{``An intersection of two overlapping open balls necessarily
    contains an open ball.}
\end{exr}

\begin{lm}[source=Kabir]
    Given a metric space ($X, d$), for any $x\in X, R>0$ and for any $y\in
    B_R(x)$, there exists $r>0$ such that $B_r(y)\subseteq B_R(x)$. \npgh

    Proof: Fix $x\in X$ and $R>0$ to be arbitrary. Consider the ball $B_R(x)
    subseteq X$ as below:
    \begin{gather*}
        B_R(x) = \set{s\in X: d(x,s)< R}
    \end{gather*}
    Pick an arbitrary $y\in B_R(x)$, we have that $d(x,y)<R$. Pick $r=R-d(x,y)$.
    We want to show that $B_r(y)\subseteq B_R(x)$. Proceeding with a one-way
    inclusion proof, we let $t\in B_r(y)$ and see the following:
    \begin{flalign*}
        t\in B_r(y) \iff& d(t,y) < r = R- d(x,y)\\
         \iff& d(t,y) < R - d(x,y) \\
         \implies& d(t,y) + d(y,x) < R \\
         \implies   & d(t,x) \leq d(t,y) + d(y,x) < R\ \
         [\because \text{Triangle inequality}]\\
          \implies  & d(t,x) < R\\\implies&  t\in B_R(x)
    \end{flalign*}
    Which is what we needed to show, therefore the given lemma is true.
\end{lm}

\begin{pf}[source=Kabir]
    Consider a metric space $(X,d)$, we need to show that for any two overlapping
    balls, we can contain a ball in the intersection. To formalize the notion, we
    need to show that for any $x,y\in X$ and $R_1,R_2>0$ such that $B_{R_1}(x)
    \cap B_{R_2}(y)\neq\eset$, there exists $t\in B_{R_1}(x)\cap B_{r_2}(y)$ and
    $r>0$ such that $B_r(t)\subseteq B_{R_1}(x)\cap B_{R_2}(y)$. \vsp
    %
    Given our two arbitrary balls $B_{R_1}(x),\ B_{R_2}(y)$ such that $B_{R_1}(x)
    \cap B_{R_2}(y) \neq\eset$, we have at least one element in the intersection,
    pick this as our $t\in B_{R_1}(x) \cap B_{R_2}(y)$ which means that
    $t\in B_{R_1}(x)$ and $t\in B_R{_2}(y)$. \vsp
    %
    Applying our lemma earlier, we have for $t\in B_{R_1}$, there exists $r_1>0$
    such that $B_{r_1}(t)\subseteq B_{R_1}(x)$. Likewise, we may construct
    $r_2>0$ such that $B_{r_2}(t) \subseteq B_{R_2}(y)$.
    Without loss of generality, let $r_1\leq r_2$, we have trivially that
    $B_{r_1}(t)\subseteq B_{r_2}(t)$. Using this information along with the fact
    that $B_{r_1}(t) \subseteq B_{R_1}(x)$, we have the following:
    \begin{gather*}
        B_{r_1}(t)\subseteq B_{r_2}(t)\subseteq B_{R_2}(y)\implies B_{r_1}(t)
        \subseteq B_{R_2}(y)
    \end{gather*}
    Keeping this in mind, we conclude that $B_{r_1}\subseteq B_{R_1}(x)\cap
    B_{R_2}(y)$, which is what we needed to show.
\end{pf}

\newpage
\subsubsection{Week 3 | Exercise 3.9}

The following is Exercise 3.9 from Week 3.
This was solved together with Hana.

\begin{exr}[num=3.9]
    Let $ (x_{n}), (y_{n}) $ be sequences in a \textit{normed vector space} $
    (X, \norm{\cdot}) $. Suppose that $ x_{n} \rightarrow p $ and $ y_{n}
    \rightarrow q $, and let $ c \in \bb{R} $. Prove that
    \begin{equation*}
        \lim_{n\rightarrow\infty} (x_{n}+cy_{n}) = p+cq
    \end{equation*}
\end{exr}

\begin{pf}[source=Hana]
    Since $ x_{n} \rightarrow p $, choose $ \ep_{1} > 0, N_{1} \in \bb{N} $, and
    since $ y_{n} \rightarrow q $, choose $ \ep_{2} > 0, N_{2} \in \bb{N} $, and
    let $ \ep = \ep_{1} + \abs{c} \ep_{2} $ and $ N = \max(N_{1}, N_{2}) $. Then:
    \begin{gather*}
        \norm{x_{n}+cy_{n} - (p+cq)} = \norm{x_{n}-p+cy-cq} \\
        \leq \norm{x_{n}-p} + \norm{cy_{n}-cq} \\
        = \norm{x_{n}-p} + \abs{c}\norm{y_{n}-cq}
    \end{gather*}
    By the limits of $ x_{n} $ and $ y_{n} $ respectively,
    \begin{equation*}
        \leq \ep_{1} + \abs{c}\ep_{2} = \ep
    \end{equation*}
    Therefore, for all $ \ep > 0 $ and $ n \geq N \in \bb{N} $, we have:
    \begin{equation*}
        \norm{x_{n} + cy_{n}-(p+cq)} < \ep
    \end{equation*}
    By definition, we get that
    \begin{equation*}
        \lim_{n\rightarrow\infty}(x_{n}+cy_{n}) = p+cq
    \end{equation*}
\end{pf}

\newpage
\subsubsection{Week 5 | Exercise [Misc]}

The following is a miscellaneous theorem from Week 5.
This was solved together with Sultan.

\begin{exr}[num=5.257]
    Let $ (X, d_{X}) $ and $ (Y, d_{Y}) $ be metric spaces, and let
    $ f:X\rightarrow Y $ be a continuous function. If $ S \subseteq X $ is
    compact, then $ f(S) $ is bounded. Furthermore, $ f(S) $ attains its
    diameter: there exist points $ a, b \in f(S) $ such that $ d_{Y}(a, b)
    = \trm{diam}(f(S))$.
\end{exr}

\begin{pf}[source=Sultan]
    We know from 4.24 that the continuous image of a compact set is compact.
    This automatically means that $ f(S) $ is bounded.
    We just need to show that $ f(S) $ attains its diameter.
    Since $ f(S) $ is compact, $ f(S) \times f(S) $ is also compact.
    Since compact implies clustering, $ f(S) \times f(S) $ is also clustering.
    Let $ D = \trm{diam}(f(S)) $. We can use the clustering property to
    construct a sequence in $ f(S) \times f(S) $ which converges to $ (a, b) $,
    where $ d_{Y}(a, b) = D $.
    To do this, for any $ n $, simply take $ a_{n}, b_{n} \in f(S) $ such that
    \begin{equation*}
        D - d_{Y}(a_{n},b_{n}) < \frac{1}{n}
    \end{equation*}
    Note that we can do this because the diameter is a supremum of distances,
    and we can always find values arbitrarily close to a supremum.
    We have now defined the sequence $ (a_{n}, b_{n}) $, and we can take a
    convergent subsequence $ (a_{m}, b_{m}) $.
    Let's say the subsequence converges to $ (a, b) $.
    Since product sequences converge component-wise, this means that
    $ (a_{m}) \rightarrow a $ and $ (b_{m}) \rightarrow b $. \vsp
    %
    We now need to show that $ d_{Y}(a, b) = D $.
    We can do this by showing that, for any $ \ep > 0 $, we get
    $ d_{Y}(a, b) > D - \ep $. Let $ \ep > 0 $ be given.
    Due to convergence, we can find a $ I, J $ such that if $ i > I $ and
    $ j > J $, then
    \begin{equation*}
        d_{Y}(a_{i}, a) < \frac{\ep}{3} \quad \trm{and} \quad
        d_{Y}(b_{j}, b) < \frac{\ep}{3}
    \end{equation*}
    We can also find an $ M $ such that if $ m > M $, then
    \begin{equation*}
        d_{Y}(a_{m}, b_{m}) > D - \frac{\ep}{3}
    \end{equation*}
    Let $ K = \max(I,J,M) $. Take any $ k > K $. Then,
    \begin{align*}
        d_{Y}(a,b)+\frac{\ep}{3}+\frac{\ep}{3}&\geq d_{Y}(a_{k}, a)+d_{Y}(a,b)
        +d_{Y}(b,b_{k})\\
        &\geq d_{Y}(a_{k},b_{k}) \\
        &> D-\frac{\ep}{3}
    \end{align*}
    which implies that
    \begin{equation*}
        d_{Y}(a,b)>D-\ep
    \end{equation*}
    which is what we wanted to show. Therefore, $ d_{Y}(a,b)=D $.
\end{pf}

\newpage
\subsubsection{Week 6 | Exercise 6.48}

The following is Exercise 6.48 from Week 6.
This was solved together with Alan.
% TODO: fix spacing

\begin{exr}[num=6.48]
    \text{[Show that]} if $(X, \Vert \cdot \Vert_X)$ and $(Y, \Vert \cdot
    \Vert_Y)$ are finite dimensional normed vector space, then every mapping
    $f : X \rightarrow Y$ is bounded.
\end{exr}

\begin{pf}[source=Alan]
    Let $\beta = \{v_1,\cdots,v_n\}$ be a basis for $X$.
    Then for all $x \in X$, there exists $c_1,\cdots,c_n \in \mathbb{R}$ such
    that $x = \sum_{k=1}^n c_iv_i$, where:
    \begin{align*}
        \Vert f(x) \Vert_Y &= \Vert f(\sum_{k=1}^n c_kv_k) \Vert_Y \\
        &= \Vert \sum_{k=1}^n c_kf(v_k) \Vert_Y \\
        &\leq  \sum_{k=1}^n |c_k| \Vert f(v_k) \Vert_Y \\
        &\leq (\max_{1 \leq k \leq n} \Vert f(v_k) \Vert_Y) \sum_{k=1}^n |c_k| \\
        &= (\max_{1 \leq k \leq n} \Vert f(v_k) \Vert_Y) \Vert x
        \Vert_{\text{imp}} \\
        &\leq (\max_{1 \leq k \leq n} \Vert f(v_k) \Vert_Y) M
        \cdot \Vert x \Vert_X
    \end{align*}
    Therefore, $f$ is a bounded linear operator, as required.
\end{pf}

\newpage
\subsubsection{Week 10 | Exercise 8.12(c)(ii)}
% TODO: fix spacing

The following is Proposition 8.12(c)(ii) from Week 10.
This was solved together with Sanchit.
\vspace{-2ex}
\begin{exr}[num=8.12(c)(ii)]
    Fix $k\in \{1,\ldots,n\}$. Use the single-variable MVT to show that there
    exists a point $q_k\in U$ such that
    \[ \dfrac{f(p_k)-f(p_{k-1})}{h_k} = \dfrac{\partial f}{\partial x_k}(q_k) \]
\end{exr}
\vspace{-2ex}
\begin{pf}[source=Sanchit]
    Define $\varphi_k:[0, h_k] \to \bb{R}$ by
    \[
        \varphi_k(t) = f(p_{k-1}+te_k)
    \]
    Then one sees that
    \begin{align*}
        \varphi_k(h_k) &= f(p_{k-1}+h_ke_k) = f(p_k) \\
        \varphi_k(0) &= f(p_{k-1}+0e_k) = f(p_k)
    \end{align*}
    Now applying MVT to $\varphi_k$ we see there exists some $c \in [0, h_k]$
    such that
    \begin{equation*}
        \varphi_k'(c) = \frac{\varphi_k(h_k) - \varphi_k(0)}{h_k - 0}
        = \frac{f(p_k) - f(p_{k-1})}{h_k}
    \end{equation*}
    Note that we can apply the first principle definition of a derivative of a
    single variable to see that
    \begin{align*}
        \varphi_k'(c) &= \lim_{t\to0} \frac{|\varphi_k(c+t)-\varphi_k(c)|}{t} \\
        &= \lim_{t \to 0} \frac{|f(p_{k-1}+(c+t)e_k)-f(p_{k-1}+ce_k)|}{t} \\
        &= \lim_{t \to 0} \frac{|f(p_{k-1}+ce_k+te_k)-f(p_{k-1}+ce_k)|}{t} \\
        &= D_{e_k}f(p_{k-1}+ce_k) \qquad \qquad
        \text{By definition of directional derivative} \\
        &= \frac{\partial f}{\partial x_k}f(p_{k-1}+ce_k)
    \end{align*}
    Defining $q_k = p_{k-1}+ce_k$ one sees that
    \[ \dfrac{f(p_k)-f(p_{k-1})}{h_k} = \dfrac{\partial f}{\partial x_k}(q_k) \]
\end{pf}

\newpage
\subsubsection{Week 11 | Exercise 9.5}

The following is Exercise 9.5 from Week 11.
This was solved together with Alan.

\begin{exr}[num=9.5]
    Prove that for any quadratic form $ Q $ on $ \bb{R}^{n} $, there exists
    a \textit{unique} symmetric matrix $ A $ such that:
    \begin{equation*}
        Q(\vec{x}) = \vec{x}^{T}A\vec{x}
    \end{equation*}
\end{exr}

\begin{pf}[source=Alan]
    Let $Q(x_1, \cdots, x_n) = \sum_{i=1}^n \sum_{j=1}^n c_{i,j} x_i, x_j$ be
    a quadratic form.
    Consider the matrix $C \in \mathcal{M}_{n \times n}(\mathbb{R})$ where:
    $$
    C_{i,j} = c_{i,j}
    $$

    First, I claim that $Q(\vec{x}) = \vec{x}^T C \vec{x}$. Indeed:
    \begin{align*}
    \vec{x}^T C \vec{x} &=  \langle C \vec{x}, \vec{x} \rangle_{\mathbb{R}^n} \\ 
    &=  \langle C(\sum_{j=1}^n x_j e_j), \vec{x} \rangle \\
    &=  \langle \sum_{j=1}^n x_j Ce_j, \vec{x} \rangle \\
    &=  \langle \sum_{j=1}^n x_j (\sum_{k=1}^n C_{k,j} e_k) , \vec{x} \rangle
    \quad (Ce_j \text{ is the jth column of } C) \\
    &=  \langle \sum_{j=1}^n \sum_{k=1}^n C_{k,j} c_je_k , \sum_{i=1}^n c_ie_i
    \rangle \\
    &= \sum_{i=1}^n x_i \langle \sum_{j=1}^n \sum_{k=1}^n A_{k,j} c_je_k, e_i 
    \rangle \quad (\text{linearity in the 2nd slot}) \\
    &= \sum_{i=1}^n x_i \sum_{j=1}^n \sum_{k=1}^n A_{k,j} a_j \langle e_k, e_i 
    \rangle \quad (\text{linearity in the 1st slot}) \\
    &= \sum_{i=1}^n x_i \sum_{j=1}^n  C_{i,j} x_j \quad (\text{as standard basis
        is orthonormal}) \\
    &= \sum_{i=1}^n  \sum_{j=1}^n  C_{i,j} x_jx_i \\
    &= Q(\vec{x}) \\
    \end{align*}

    Now, let $A = \frac{C + C^T}{2}$. Then note $A$ is symmetric as $A^T
    = \frac{C^T + (C^T)^T} {2} = \frac{C + C^T}{2} = A$. \vsp
    %
    Also:
    \begin{align*}
    \vec{x}^T A \vec{x} &= \langle \vec{x}, A\vec{x} \rangle \\
    &= \langle \vec{x}, \frac{C + C^T}{2} \vec{x} \rangle \\
    &= \frac{1}{2} \langle \vec{x}, C \vec{x} \rangle + \frac{1}{2}
    \langle \vec{x}, C^T \vec{x}
    \rangle \quad (\text{linearity in 2nd slot}) \\
    &= \frac{1}{2} \langle \vec{x}, C \vec{x} \rangle + \frac{1}{2} \langle C
    \vec{x}, \vec{x} \rangle \quad (\text{the adjoint of } C \text{ is } C^T) \\
    &= \frac{1}{2} \langle \vec{x}, C \vec{x} \rangle + \frac{1}{2} \langle
    \vec{x}, C \vec{x} \rangle \quad (\text{symmetry of real inner product}) \\
    &= \frac{1}{2} Q(\vec{x}) + \frac{1}{2} Q(\vec{x}) \\
    &= Q(\vec{x}) \\
    \end{align*}

    Therefore, $A$ is a symmetric matrix such that $Q(\vec{x}) = \vec{x}^T A 
    \vec{x} $. \npgh

    Next, \text{[we show that the]} symmetric matrix is unique.
    Suppose $A,B \in \mathcal{M}_{n \times n}(\mathbb{R})$ are two symmetric
    matrices such that for all $\vec{x} \in \mathbb{R}^n$:
    $$
    Q(\vec{x}) = \vec{x}^T A \vec{x} = \vec{x}^T B \vec{x}
    $$

    This means:
    \begin{gather*}
    \langle \vec{x}, A \vec{x} \rangle = \langle \vec{x}, B \vec{x} \rangle \\
    \langle \vec{x}, (A - B) \vec{x} \rangle = 0 \\
    \end{gather*}

    Since this is true for all $\vec{x} \in \mathbb{R}^n$, we have that
    $(A - B) \vec{x} = \vec{0}$, i.e. $A \vec{x} = B \vec{x}$.
    Hence, we must have $A = B$.
\end{pf}
\newpage
\subsubsection{Week 13 | Exercise 11.16}

The following is Exercise 11.16 from Week 13.
This was solved together with Joseph.
\vspace{-2ex}
\begin{exr}[num=11.16]
    True or false? Every bounded open set is Jordan measurable.
\end{exr}
\vspace{-2ex}
\begin{pf}[source=Joseph]
    False. Here is the counter-example:
    Consider the fat Cantor set $ C $ defined as follows:
    First, set $ C_{0} = [0,1] $, we see $ C_{0} $ contains $ 2^{0} $ interval.
    \\
    For each $ i \in \bN $, assume $ C_{i} $ is defined, define $ C_{i+1} $ as
    follows: remove the middle $ \frac{1}{4^{i+1}} $ open interval from each of
    the $ 2^{i} $ closed intervals in $ C_{i} $. \\
    Then, we see $ C_{i+1} $ removes $ \frac{2^{i}}{4^{i+1}} =
    \frac{2^{i}}{2^{2i+2}} $ of total length from $ C_{i} $. Thus, let
    \begin{equation*}
        C := \bigcap_{i=0}^{\infty} C_{i},
    \end{equation*}
    we see the total length being removed is
    \begin{equation*}
        \sum_{i=0}^{\infty}\frac{2^{i}}{2^{2i+2}} =
        \sum_{i=0}^{\infty}\frac{1}{2^{i+2}}=\frac{1}{2}
    \end{equation*}
    Which implies $ C $ and $ [0,1] \setminus C =: C' $ both have ``length" equal
    to $ \frac{1}{2} $. \\
    Since $ C $ is a countable intersection of closed sets, $ C $ is closed,
    which implies $ (-\infty, 0] \cup C \cup [1, \infty) $ is closed as well
    since it is a finite union of closed sets. \\
    Hence, $ C'=\bR\setminus(-\infty,0]\cup C\cup[1,\infty) $ is open. \\
    $ C' $ is also bounded by $ M = 1 $, which follows from the construction of
    $ C $. \\
    Then, we claim that $ C' $ is bounded open set but not Jordan measurable. We
    have shown it is bounded and open, it remains for us to show it is not
    Jordan measurable. \\
    We will use the lemma that, a set is Jordan measurable if and only if its
    boundary has measure zero. \\
    That is, we will show $ C' $ has boundary with positive length.
    To this end, we show the boundary of $ C' $ is precisely $ C $. \\
    First, $ C $ has empty interior, otherwise assume $ U \subseteq C $ is open
    (an open interval in $ \bR $), then for a sufficient large $ k, U $ cannot
    be contained within $ C_{k} \subseteq C $, which is a contradiction. Hence,
    $ C $ has empty interior. \\
    So, the boundary of $ C $ is $ C $ since it is closed (contains all of its
    boundary points), and by definition of complement, the boundary of $ C' $ is
    $ C $ as well. \\
    Therefore, the boundary of $ C' $ has length $ \frac{1}{2} > 0 $, hence
    $ C' $ is not Jordan measurable, as needed.
\end{pf}
\newpage
\subsubsection{Week 17 | Exercise 15.14}

The following is Exercise 15.14 from Week 17.
This was solved together with Ethan.

\begin{exr}[num=11.16]
    Let \(M\) be a (smooth) \(n\)-manifold.
    Show that each point \(p \in M\) has a relatively open neighborhood
    \(U \subseteq M\) such that \(U\) homeomorphic to \(\mathbb{R}^n\). In fact,
    show that there is a smooth regular embedding \(\varphi : \mathbb{R}^n \to
    U\) such that \(\varphi (\vec{0}) = p\).
\end{exr}

\begin{pf}[source=Ethan]
Let \(p \in M\). By our assumption, we can find a smooth regular embedding
\(\hat{\varphi}: \hat{V} \to V\), where \(\hat{V} \subseteq \mathbb{R}^n\) is
open and \(V\) is a relatively open subset of \(M\) containing \(p\).
Furthermore, there exists \(q \in \hat{V}\) such that \(\hat{\varphi}(q) = p\)
and an open ball \(B_{\max}(q, r) \subseteq \hat{V}\). Note that we make use of
an open ball with respect to the max-norm, for reasons that will become clear
soon. Let \(U = \hat{\varphi}(B_{\max}(q, r))\). We are guaranteed that \(U\) is
open because \(\hat{\varphi}\) is a homeomorphism. Now, we define a smooth
homeomorphism between \(B_{\max}(q, r)\) and \(\mathbb{R}^n\).
Let \(\Phi : \mathbb{R}^n \to B_{\max}(q, r)\) be defined by
        \[
            \Phi (\vec{x}) = \left( \frac{2}{\pi r}\arctan (x_1) + q_1, ...,
            \frac{2}{\pi r}\arctan (x_n) + q_n\right) 
        \]
        Notice that \(B_{\max}(q, r)\) is a cartesian product of intervals
        \(\prod _{i=1}^n [q_i - r, q_i + r]\). The function \(\Phi_i(x) =
        \frac{2}{\pi r}\arctan (x) + q_i\) is a well known bijection between
        \(\mathbb{R}\) and \([q_i - r, q_i + r]\), so it is clear that \(\Phi\)
        is a bijection with a continuous inverse. Moreover, each component is
        smooth, so \(\Phi\) is a smooth homeomorphism. We claim that our desired
        function \(\varphi: \mathbb{R}^n \to U\) is given by
        \[
            \varphi (x) = \hat{\varphi}(\Phi (x)).
        \]
        \(\varphi\) is a composition of smooth functions, and therefore smooth.
        \(J \Phi\) is a diagonal matrix with non-zero diagonals, so it is rank
        \(n\). It follows that the Jacobian \(J \varphi (x) = J \hat{\varphi}
        (\Phi (x)) \cdot J \Phi (x)\) is rank \(n\) for all \(x \in
        \mathbb{R}^n\). Finally, \(\varphi\) is a homeomorphism since it is a
        composition of homeomorphisms (so \(U \simeq \mathbb{R}^n\), as needed).
        Thus we can conclude that \(\varphi\) is a smooth regular embedding.
        As well,
        \[
            \varphi (0) = \hat{\varphi}(\Phi (0)) = \hat{\varphi}(q) = p.
        \]
        and the proof is done.
\end{pf}

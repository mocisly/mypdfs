\subsection{Proofs of Selected Theorems}

\begin{thm}[num=6.4] % 6.41
    Let $ (X, \norm{\cdot}) $ be an $ n $-dimensional normed vector space.
    Then there is a linear homeomorphism $ \Phi: (X, \norm{\cdot}) \gto
    (\bR^{n}, \norm{\cdot}_{2}) $.
\end{thm}

\begin{lm}
    Every norm is equivalent to the 1-norm, and thus every norm is equivalent.
    \vsp
    %
    To see this, let $ \beta=\set{b_{1},\dots,b_{n}} $ be a basis. Then:
    \begin{equation*}
        \norm{x} = \norm{\sum_{i=1}^{n}x_{i}b_{i}} \leq \sum_{i=1}^{n}
        \abs{x_{i}}\norm{b_{i}} \leq n\cdot\max_{i}\norm{b_{i}}\sum_{i=1}^{n}
        \abs{x_{i}} = n\cdot\max_{i}\norm{b_{i}}\norm{x}_{1}
    \end{equation*}
    Next, to find $ m $, we define the function $ f:S^{n-1} \gto \bR $:
    \begin{equation*}
        f(x) \ = \ \frac{\norm{x}}{\norm{x}_{1}}
    \end{equation*}
    Note that since norms are positive, $ f $ is positive. Furthermore,
    $ S^{n-1} $ is compact in $ \bR^{n} $, so by EVT it attains a minimum $ m $.
    We claim that $ m\norm{x}_{1} \leq \norm{x} $ for all $ x $.
    Note that if $ x = 0 $, then trivially $ m\norm{x}_{1} \leq \norm{x} = 0 $.
    Otherwise:
    \begin{equation*}
        m \leq f \left( \frac{x}{\norm{x}_{1}} \right) =
        \frac{\norm{\frac{x}{\norm{x}_{1}}}}{\norm{\frac{x}{\norm{x}_{1}}}_{1}} =
        \frac{\norm{x}}{\norm{x}_{1}}
    \end{equation*}
    and the result follows.
\end{lm}

\newpage
\begin{pf}
    Consider the map $ \Phi(x) = (x_{1},\dots,x_{n}) $, and define a norm as
    $ \norm{x} \ = \ \norm{\Phi(x)}_{2} $. Then by our lemma, we have that there
    exist $ m, M $ such that:
    \begin{equation*}
        m\norm{\Phi(x)}_{2} \leq \norm{x} \leq M\norm{\Phi(x)}_{2}
    \end{equation*}
    This trivially implies that $ \Phi $ is continuous; an entirely analogous
    argument holds for the inverse as well. Thus, $ \Phi $ is a homeomorphism as
    needed.
\end{pf}

% yeaaaa mb just dont include this one chief
% well. i did it. :/
\newpage
\begin{thm}[num=7.1,title=Chain Rule]
    Suppose $ f:\bR^{n} \gto \bR^{m} $ is totally differentiable at $ p $,
    and $ g:\bR^{m} \gto \bR^{k} $ is totally differentiable at $ f(p) $. \vsp
    %
    Then $ g \circ f $ is totally differentiable at $ a $, and the derivative is
    given by:
    \begin{equation*}
        D_{g\circ f} \ = \ D_{g} \circ D_{f}
    \end{equation*}
    where $ D_{f}(x) $ is the total derivative of $ f $ at $ x $.
\end{thm}

Note that this proof omits many details regarding explicit calculations and
algebra.
\begin{pf}
    Let $ \ep > 0 $. From the definition of differentiable, we have
    $ \delta_{f}, \delta_{g} $ such that $ f, g $ are differentiable at $ a,
    f(a) $ respectively. Furthermore, note that $ f $ differentiable at $ a $
    implies $ f $ continuous at $ a $. \vsp
    %
    Set $ \delta = \min\set{\delta_{f},\delta_{g}} $. For $ h $ with $ \norm{h}
    < \delta $, we have:
    \begin{gather*}
        \frac{\norm{(g\circ f)(p+h)-(g\circ f)(p)-D_{g}\circ D_{f}(h)}}
        {\norm{h}} \vsp
        \ \leq \ \frac{\norm{(g\circ f)(p+h)-(g\circ f)(p)-D_{g}(f(p+h)-f(p))}}
        {\norm{h}} \vsp
        \ \leq \ \frac{\norm{(g\circ f)(p+h)-(g\circ f)(p)-D_{g}(f(p+h)-f(p))}}
        {\norm{h}} \vsp
        \ + \ \frac{\norm{D_{g}(f(p+h)-f(p) - D_{f}(h))}}{\norm{h}}
    \end{gather*}
    This follows from the fact that $ f $ is differentiable at $ p $. Now,
    examining the first term in the sum, since $ g $ is differentiable at
    $ f(p) $ and $ f $ continuous at $ p $, we conclude:
    \begin{equation*}
        \frac{\norm{(g\circ f)(p+h)-(g\circ f)(p)-D_{g}(f(p+h)-f(p))}}
        {\norm{h}} \ < \ \frac{\ep}{2}
    \end{equation*}
    Similarly, since $ f $ differentiable at $ p $ and $ D_{f} $ is bounded:
    \begin{equation*}
        \frac{\norm{D_{g}(f(p+h)-f(p) - D_{f}(h))}}{\norm{h}} \ < \ \frac{\ep}{2}
    \end{equation*}
    Thus, since we can bound each term by $ \frac{\ep}{2} $, we get that the sum
    is bounded by $ \ep $, and thus $ D_{g\circ f} = D_{g}\circ D_{f} $ as
    needed.
\end{pf}

\newpage
\begin{thm}[num=7.2,title=Differentiability Theorem]
    Let $ U \subseteq \bR^{n} $ be open, and $ f:U \gto \bR $ a function.
    Then $ f $ is continuously differentiable iff all partial derivatives
    $ \dfrac{\p f}{\p x_{i}}:U \gto \bR $ exist and are continuous on $ U $.
\end{thm}

\begin{pf}[title=Forward direction]
    We show the directional derivative with respect to an arbitrary $ v $ is
    continuous; the result holds by picking $ v = x_{i} $. \vsp
    %
    Fix an arbitrary $ v $; WLOG, suppose $ v \neq 0 $. Then, $ f' $ is
    continuous, and $ D_{v}f(p) = f'(p)(v) $. We show that $ f'(p)(v) $ is
    continuous as a function of $ p $.
    \vsp
    %
    Let $ \ep > 0 $. Since $ f' $ continuous, then there exists $ \delta > 0 $
    such that:
    \begin{equation*}
        \norm{x-y} < \delta \ \implies \ \norm{f'(x)-f'(y)}_{\trm{op}} <
        \frac{\ep}{\norm{v}}
    \end{equation*}
    Then, we have that:
    \begin{align*}
        \norm{f'(x)(v)-f'(y)(v)} \ = \ & \norm{f'(x)\left( v\frac{\norm{v}}
        {\norm{v}} \right)-f'(y)\left( v\frac{\norm{v}}{\norm{v}} \right)} \vsp
        \ = \ & \norm{v}\cdot\norm{f'(x)\left( \frac{v}{\norm{v}} \right)-
        f'(y)\left( \frac{v}{\norm{v}} \right)} \vsp
        \ \leq \ & \norm{v}\cdot\norm{f'(x)-f'(y)}_{\trm{op}} \vsp
        \ < \ & \norm{v}\cdot\frac{\ep}{\norm{v}} \\
        \ = \ & \ep
    \end{align*}
    Thus, $ f'(p)(v) $ continuous wrt $ p $, and so $ D_{v}f(p) $ continuous as
    needed.
\end{pf}

\begin{pf}[title=Backward direction]
    Suppose each partial derivative exists and is continuous on $ U $. We show
    that a total derivative for $ f $ exists and is continuous. \vsp
    %
    Fix $ p \in U $, and define $ L_{p}:U\gto\bR $ as:
    \begin{equation*}
        L_{p}(v) \ = \ \sum_{i=1}^{n}v_{i}\frac{\p f}{\p x_{i}}
    \end{equation*}
    Linearity is easy to see, and continuity follows from each partial being
    continuous. Next, we will need a sequence of points $ p_{0},\dots,
    p_{n} $ which we will define as:
    \begin{equation*}
        p_{0} = p \qquad p_{i}=p_{i-1}+h_{i}e_{i} \qquad
        p_{k} = p_{0}+h_{1}e_{1}+\dots +h_{k}e_{k}
    \end{equation*}
    for some vector $ h $. Notice that we can fix $ \delta_{1} > 0 $ such that:
    \begin{equation*}
        \norm{p_{k}-p_{0}}= \sum_{i=1}^{k}h_{i}e_{i} \leq \norm{h}_{\trm{max}}
        < \delta_{1}
    \end{equation*}
    In particular, we can fix $ \delta_{1} $ such that each $ p_{i} \in U $.
    Now, fix $ k \in \set{1,\dots,n} $; we show that there exists $ q_{k} \in
    U $ such that:
    \begin{equation*}
        \frac{f(p_{k})-f(p_{k-1})}{h_{k}} \ = \ \frac{\p f}{\p x_{k}}(q_{k})
    \end{equation*}
    Indeed, define $ \vphi_{k}:[0,h_{k}]\gto\bR $ as:
    \begin{equation*}
        \vphi_{k}(t) \ = \ f(p_{k-1}+te_{k})
    \end{equation*}
    In particular, note that $ \vphi_{k}(0)=f(p_{k-1}) $ and $ \vphi_{k}(h_{k})
    = f(p_{k}) $. Then, by MVT, there exists $ c \in [0, h_{k}] $ such that:
    \begin{equation*}
        \vphi_{k}'(c) \ = \ \frac{\vphi_{k}(h_{k})-\vphi_{k}(0)}{h_{k}}
        \ = \ \frac{f(p_{k})-f(p_{k-1})}{h_{k}} \ = \
        \frac{\p}{\p x_{k}}f(p_{k-1}+ce_{k})
    \end{equation*}
    The last equality comes from expanding the limit definition of derivative.
    Thus, setting $ q_{k} = p_{k-1}+ce_{k} $, our result follows. \vsp
    %
    We are now ready to show that $ L_{p} $ is a linear approximation.
    Let $ \ep > 0 $. By uniform continuity, note that we have $ \delta_{2} > 0 $
    such that:
    \begin{equation*}
        \norm{a-b}<\delta_{2} \ \implies \ \abs{\frac{\p f}{\p x_{i}}(a-b)}<\ep
    \end{equation*}
    Let $ \delta = \min(\delta_{1},\delta_{2}) $ and $ \norm{h} < \delta $. Then:
    \begin{align*}
        \frac{\abs{f(p+h)-f(p)-L_{p}(h)}}{\norm{h}} & \ = \
        \frac{\abs{f(p_{n})-f(p_{0})-L_{p}(h)}}{\norm{h}} \vsp
        \ = \ & \frac{\abs{\sum_{i=1}^{n}\left( f(p_{i})-f(p_{i-1})-h_{i}
        \frac{\p f}{\p x_{i}}(p)\right)}}{\norm{h}} \vsp
        \ \leq \ & \frac{1}{\norm{h}}\sum_{i=1}^{n}\abs{f(p_{i})-f(p_{i-1})-
        h_{i}\frac{\p f}{\p x_{i}}(p)} \vsp
        \ = \ & \frac{1}{\norm{h}}\sum_{i=1}^{n}\abs{h_{i}\frac{\p f}{\p x_{i}}
        (q_{i})-h_{i}\frac{\p f}{\p x_{i}}(p)} \vsp
        \ = \ & \frac{1}{\norm{h}}\sum_{i=1}^{n}\abs{h_{i}}\abs{\frac{\p f}
        {\p x_{i}}(q_{i}-p)} \vsp
        \ = \ & \frac{1}{\norm{h}}\sum_{i=1}^{n}\abs{h_{i}}\abs{\frac{\p f}
        {\p x_{i}}\left( ce_{i}+\sum_{j=1}^{i-1}h_{j}e_{j} \right)} \vsp
        \ < \ & \frac{1}{\norm{h}}\sum_{i=1}^{n}\abs{h_{i}}\cdot\ep \vsp
        \ = \ & \frac{\norm{h}}{\norm{h}}\cdot\ep \vsp
        \ = \ & \ep
    \end{align*}
    Note the norm of the argument of the partial derivative is less than
    $ \delta $, and so the inequality follows from continuity of the partials.
    Thus, we see that $ L_{p} $ is a linear approximation to $ f $ at $ p $. \vsp
    %
    It remains to show that $ L_{p} $ is continuous. To see this, fix $ \ep > 0 $
    and note that $ f' $ is a gradient vector, and thus a $ 1\times n $ matrix.
    We have:
    \begin{equation*}
        [L_{p}]_{i} \ = \ \frac{\p f}{\p x_{i}}(p)
    \end{equation*}
    Since the partial is continuous, we have $ \delta_{i} > 0 $ such that:
    \begin{equation*}
        \norm{h_{i}} < \delta_{i} \ \implies \
        \norm{\frac{\p f}{\p x_{i}}(p_{i}+h_{i}) - \frac{\p f}{\p x_{i}}(p_{i})}
        < \frac{\ep}{2^{i}}
    \end{equation*}
    Let $ \delta = \min_{i}\set{\delta_{i}} $ and $ \norm{h}<\delta $. Note that
    $ \norm{f'(p+h)-f'(p)}_{\trm{op}} \ = \ \norm{L_{h}}_{\trm{op}} $, and for
    any $ \norm{v} \leq 1 $, we have that:
    \begin{gather*}
        \norm{L_{h}(v)}^{2} \ = \ \left( \sum_{i=1}^{n}a_{i}v_{i} \right)^{2}
        \ = \ \sum_{i=1}^{n}a_{i}^{2}\cdot\sum_{i=1}^{n}v_{i}^{2} \vsp
        \ = \ \sum_{i=1}^{n}\left(\frac{\p}{\p x_{i}}f(h)\right)^{2}\norm{v}^{2}
        \ \leq \ \sum_{i=1}^{n}\norm{\frac{\p}{\p x_{i}}f(h)}_{\trm{op}}^{2} \vsp
        \ \implies \ \norm{L_{h}(v)} \ \leq \
        \sqrt{\sum_{i=1}^{n}\norm{\frac{\p}{\p x_{i}}f(h)}^{2}_{\trm{op}}}
        \ \leq \ \sqrt{\ep^{2}} \ = \ \ep
    \end{gather*}
    It follows that $ f' $ is continuous at $ p $. Since this holds for all
    $ p \in U $, then we have that $ f' $ is continuous as needed.
\end{pf}

\newpage
\begin{thm}[num=7.4,title=Inverse Function Theorem]
    Let $ U \subseteq \bR^{n} $ be open, and $ \Phi: U \gto \bR^{n} $ a
    $ C^{1} $ mapping. Suppose $ p_{0} \in U $ is a point such that
    $ \Phi'(p_{0}): \bR^{n} \gto \bR^{n} $ is an isomorphism. Then, there
    exists an open neighbourhood $ U_{0} $ of $ p_{0} $ such that:
    \begin{enumerate}
        \item The restriction $ \Phi\rvert_{U_{0}} $ is injective
        \item The image $ V_{0} = \Phi(U_{0}) $ is open in $ \bR^{n} $
        \item $ \Phi^{-1}:V_{0} \gto U_{0} $ is $ C^{1} $
        \item For each $ p \in U_{0} $, the derivative of $ \Phi^{-1} $ at
            $ q = \Phi(p) $ is:
            \begin{equation*}
                (\Phi^{-1})'(q) \ = \ \phi'(p)^{-1}
            \end{equation*}
    \end{enumerate}
\end{thm}

\begin{pf}[title=Item 1]
    Set $ L = \Phi'(p_{0}) $, and for some $ y \in \bR^{n} $, define a function:
    \begin{equation*}
        \alpha(p) = p + L^{-1}(y-\Phi(p))
    \end{equation*}
    Then $ \Phi(p) = y $ if and only if $ p $ is a fixed point of $ \alpha $.
    Indeed:
    \begin{gather*}
        \Phi(p) \ = \ y \ \implies \ \alpha(p) \ = \ p + L^{-1}(y - \Phi(p))
        \ = \ p + L^{-1}(0) \ = \ p \vsp
        \alpha(p) = p \ \implies \ L^{-1}(y-\Phi(p)) = 0
        \ \implies \ y - \Phi(p) = 0 \ \implies \ \Phi(p) = y
    \end{gather*}
    Next, we notice that:
    \begin{equation*}
        \norm{\alpha'(p)}_{\trm{op}} \ = \ \norm{I - L^{-1}(\Phi'(p))}_{\trm{op}}
        \ = \ \norm{L^{-1}(L-\Phi'(p))}_{\trm{op}} \ \leq \
        \norm{L^{-1}}_{\trm{op}}\norm{\Phi'(p)-L}_{\trm{op}}
    \end{equation*}
    Taking $ \delta = \frac{1}{2\norm{L^{-1}}} $, then continuity of $ \Phi $
    gives us:
    \begin{equation*}
        \norm{\alpha'(p)} \ \leq \ \norm{L^{-1}}\cdot\norm{\Phi'(p)-\Phi'(p_{0})}
        \ \leq \ \frac{\norm{L^{-1}}}{2\norm{L^{-1}}} \ = \ \frac{1}{2}
    \end{equation*}
    By MVT, there exists some $ c \in U_{0} $ such that:
    \begin{equation*}
        \norm{\alpha(p)-\alpha(q)} \ = \ \norm{\alpha'(c)}\cdot\norm{p-q}
        \ \leq \ \frac{1}{2}\norm{p-q}
    \end{equation*}
    Thus, we conclude that $ \alpha $ is a contraction mapping, and thus has a
    unique fixed point. Since $ \Phi(p)=y $ iff $ p $ is a fixed point of
    $ \alpha $, then must $ \Phi $ be injective.
\end{pf}

\begin{pf}[title=Item 2]
    Fix $ y = q_{0} \in V_{0} $ and write $ q_{0} = \Phi(p) $ for some
    $ p \in U_{0} $. Then, there exists some $ r > 0 $ such that
    $ C = \bar{B}(p, r) \subseteq U_{0} $. Notice that for any $ x \in C $:
    \begin{equation*}
        \norm{\alpha(x)-\alpha(p)} \ = \ \norm{\alpha(x)-p} \ \leq \ \frac{1}{2}
        \norm{x-p}
    \end{equation*}
    So we have that $ \alpha(C) \subseteq C $, and since it is a contraction
    mapping, thus has a unique fixed point in $ C $. Thus, the result follows.
\end{pf}

\begin{pf}[title=Items 3 and 4]
    TBA
\end{pf}

\newpage
\begin{thm}[num=7.5,title=Implicit Function Theorem]
    Let $ U \subseteq \bR^{n+k} $ be open and $ F: U \gto \bR^{k} $ be
    $ C^{1} $. Suppose $ (a, b) \in U $ for some $ a \in \bR^{n}, b \in \bR^{k} $
    such that:
    \begin{equation*}
        F(a, b) = 0 \qquad \det(B) = \det
        \begin{bmatrix}
            \p_{y_{1}}F_{1}(a,b) & \dots & \p_{y_{k}}F_{1}(a,b) \\
            \vdots & \ddots & \vdots \\
            \p_{y_{1}}F_{k}(a,b) & \dots & \p_{y_{k}}F_{k}(a,b)
        \end{bmatrix}
        \neq 0
    \end{equation*}
    where $ F = (F_{1},\dots,F_{k}) $. \vsp
    %
    Prove there exists an open set $ V \subseteq \bR^{n} $ containing $ a $, and
    a unique $ C^{1} $ function $ f:V \gto \bR^{k} $ such that $ f(a) = b $, and
    for all $ x \in V $ we have $ F(x,f(x)) = 0 $.
\end{thm}

\begin{pf}
    Define a function $ G:U \gto \bR^{n+k} $ as:
    \begin{equation*}
        G(x,y) = (x, F(x,y))
    \end{equation*}
    Notice that each component of $ G $ is $ C^{1} $, thus $ G $ is $ C^{1} $.
    Furthermore, we have:
    \begin{equation*}
        G'(a,b) \ = \ \left[
        \begin{tabular}{CCC|CCC}
            1 & \cdots & 0 & 0 & \cdots & 0 \\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            0 & \cdots & 1 & 0 & \cdots & 0 \\ \hline
            \p_{x_{1}}F_{1}(a,b) & \cdots & \p_{x_{n}}F_{1}(a,b) &
            \p_{y_{1}}F_{1}(x,y) & \cdots & \p_{y_{n}}F_{1}(a,b) \\
            \vdots & \ddots & \vdots & \vdots & \ddots & \vdots \\
            \p_{x_{1}}F_{k}(a,b) & \cdots & \p_{x_{n}}F_{k}(a,b) &
            \p_{y_{1}}F_{k}(a,b) & \cdots & \p_{y_{n}}F_{k}(a,b)
        \end{tabular}
        \right]
    \end{equation*}
    Notice that $ \det(G'(a,b)) = \det(B) $ as given in the statement, and so we
    have that $ \det(G'(a,b)) \neq 0 $. By Inverse Function Theorem, we have open
    sets $ U_{0}, V_{0} $ containing $ (a,b) $ and $ G(a,b) = (a,0) $
    respectively, where $ G\rvert_{U_{0}}:U_{0}\gto V_{0} $ is a diffeomorphism.
    We write:
    \begin{equation*}
        (G\rvert_{U_{0}})^{-1}(x,y) \ = \ (A(x,y),B(x,y))
    \end{equation*}
    \newpage
    Write $ H = G\rvert_{U_{0}} $. Since $ H $ invertible, then for each
    $ (x,y) \in V_{0} $ we have that:
    \begin{equation*}
        (x,y) \ = \ (H \circ H^{-1})(x,y) \ = \ H(A(x,y),B(x,y))
        \ = \ (A(x,y), F(A(x,y),B(x,y)))
    \end{equation*}
    Therefore, we must have that $ A(x,y) = x $ and $ F(x,B(x,y)) = y $. \vsp
    %
    Pick $ r > 0 $ such that $ B((a,0),r) \subseteq V_{0} $ and define the set:
    \begin{equation*}
        W = \pi_{n}(B((a,0),r))
    \end{equation*}
    where $ \pi_{n} $ is the projection onto the first $ n $ coordinates.
    Then $ W $ is an open set containing $ a $. Additionally:
    \begin{equation*}
        x \in W \ \implies \ (x,0) \in V_{0}
    \end{equation*}
    Thus, define $ f:W \gto \bR^{k} $ by $ f(x) = B(x, 0) $. Since $ B $ is
    $ C^{1} $, it follows that $ f $ is $ C^{1} $. Furthermore:
    \begin{equation*}
        0 \ = \ F(x, B(x, 0)) \ = \ F(x,f(x))
    \end{equation*}
    We also have that $ (a,b) = H^{-1}(a,0) = (a,B(a,0)) = (a,f(a)) $, and so
    we see that $ b = f(a) $ as needed.
\end{pf}

\newpage
\begin{thm}[num=9.2,title=Change of Variables]
    Let $ U, V \subseteq \bR^{n} $ be open, $ \Phi: U \gto V $ a
    diffeomorphism, and $ E \subseteq \bR^{n} $ measurable. Then $ \Phi(E) $ is
    also measurable, and its measure is given by:
    \begin{equation*}
        \mu(\Phi(E)) \ = \ \int_{E}\abs{\det J\Phi}
    \end{equation*}
    As a corollary, we have that for an integrable function $ f:B \gto \bR $
    where $ B = \Phi(E) $:
    \begin{equation*}
        \int_{B}f \ = \ \int_{E}(f \circ \Phi)\cdot\abs{\det J\Phi}
    \end{equation*}
\end{thm}

\begin{pf}[title=Linear case]
    Note that it suffices to prove the statement for a box $ E $, as measurable
    sets can be approximated by boxes. Furthermore, it suffices to prove the
    statement for $ E = [0,1]^{n} $, as an arbitrary box can be scaled and
    shifted without issue. \vsp
    %
    Suppose $ \Phi = T $ is linear. We first consider the three cases where $ T $
    is an elementary matrix. \vsp
    %
    If $ T $ is obtained by swapping two rows, then $ T(E) = E $, and we have:
    \begin{equation*}
        \mu(T(E)) \ = \ \mu(E) \ = \ \abs{-1}\mu(E)
    \end{equation*}
    If $ T $ is obtained by scaling a row by a nonzero scalar $ c $, then WLOG we
    have:
    \begin{equation*}
        T(E) \ = \ [0,1]^{n-1} \times [0, c] \qquad
        \mu(T(E)) = \vol(T(E)) = \abs{c}\mu(E) = \abs{\det(T)}\mu(E)
    \end{equation*}
    Now, consider the case where $ T $ is obtained by adding the $ j $-th row to
    the $ i $-th row. For any $ n $, define polyboxes on $ T(E) $ as:
    \begin{equation*}
        \bar{P}_{n} \ = \ \bigcup_{i=1}^{n}\left[\frac{i-1}{n},1+\frac{i}{n}
        \right]\times\left[\frac{i-1}{n},\frac{i}{n}\right] \qquad
        \underline{P}_{n} \ = \ \bigcup_{i=1}^{n}\left[\frac{i}{n},1-\frac{i-1}
        {n}\right]\times\left[\frac{i-1}{n},\frac{i}{n}\right]
    \end{equation*}
    where $ \bar{P}_{n} $ is an outer polybox and $ \underline{P}_{n} $ is an
    inner polybox. We see that as $ n \sto \infty, \bar{P}_{n} \sto T(E) $, and
    analogously for the inner polyboxes. Then:
    \begin{gather*}
        \mu(\bar{P}_{n}) \ = \ \vol(\bar{P}_{n}) \ = \ \sum_{i=1}^{n}\left(
        1+\frac{1}{n} \right)\frac{1}{n} \ = \ 1 + \frac{1}{n} \vsp
        \mu(\bar{P}_{n}) \ = \ \vol(\bar{P}_{n}) \ = \ \sum_{i=1}^{n}\left(
        1-\frac{1}{n} \right)\frac{1}{n} \ = \ 1 - \frac{1}{n}
    \end{gather*}
    Furthermore, we have that:
    \begin{equation*}
        \mu_{*}(T(E)) \ = \ \lim_{n\sto\infty}\mu(\bar{P}_{n}) \qquad
        \mu^{*}(T(E)) \ = \ \lim_{n\sto\infty}\mu(\underline{P}_{n})
    \end{equation*}
    Therefore, we conclude that $ \mu(T(E)) = 1 $, and so the statement holds.
    \vsp
    %
    Note that if $ T $ is non-invertible, then $ \det(T) = 0 $, and
    $ \mu(T(E)) = 0 $ as it is a closed proper subspace, so the statement also
    holds. Now, suppose that $ T $ is an invertible, non-elementary matrix. Then,
    we can write $ T = T_{1}T_{2}\cdots T_{k} $ as the product of $ k $
    elementary matrices. We proceed by strong induction. \vsp
    %
    Suppose $ k=1 $. This case is shown above. Thus, fix $ k $, and assume the
    statement holds for all $ 1 \leq k' \leq k $. We have that:
    \begin{equation*}
        T \ = \ T_{1}T_{2}\cdots T_{k+1} \qquad
        \det(T) \ = \ \det(T_{1})\det(T_{2})\cdots \det(T_{k+1})
    \end{equation*}
    By our induction hypothesis, we have that:
    \begin{gather*}
        \mu(T(E)) \ = \ \mu((T_{1}\cdots T_{k+1})(E)) \ = \ \mu(T_{1}(E')) \vsp
        \ = \ \abs{\det(T_{1})}\mu(E') \ = \ \abs{\det(T_{1})}\cdots
        \abs{\det(T_{k+1})}\mu(E) \ = \ \abs{\det(T)}\mu(E)
    \end{gather*}
    Thus, by induction, the statement holds in the linear case as needed.
\end{pf}

\newpage
\begin{pf}[title=Simple case]
    Recall that a function $ \vphi = (\vphi_{1},\dots,\vphi_{n}) $ is called
    simple if $ \vphi_{i}(x) = x_{i} $ for all but one $ i $. We now demonstrate
    the case where $ \Phi $ is simple. \vsp
    %
    WLOG, suppose that $ \Phi(x) = (x_{1}, \dots, x_{n-1}, \vphi(x)) $. Recall
    $ E = [0,1]^{n} $; consider $ E' = [0,1]^{n-1} $. Define the function
    $ \Psi: E' \gto \bR $ as:
    \begin{equation*}
        \Psi(x') \ = \ \int_{\vphi(x',[0,1])}1\di x_{n}
    \end{equation*}
    Since $ \Phi $ is a diffeomorphism, then $ \vphi(x', \cdot) $ is $ C^{1} $
    and injective. Thus, $ \vphi(x',[0,1]) $ is an interval. Then, we use
    single-variable CoV to write $ \vphi(x',u) = x_{n} $. Then:
    \begin{equation*}
        \di x_{n} \ = \ \frac{\p \vphi}{\p u}(x',u)\di u
    \end{equation*}
    We also have:
    \begin{equation*}
        \det J\Phi \ = \ \det
        \begin{bmatrix}
            I \\ \nabla\vphi(x',u)
        \end{bmatrix}
        \ = \ \frac{\p \vphi}{\p u}(x', u)
    \end{equation*}
    Thus, we have that:
    \begin{equation*}
        \Psi(x') = \int_{\vphi(x',[0,1])}1\di x_{n} \ = \ \int_{0}^{1}
        \abs{\det(J\Phi)}
    \end{equation*}
    Using Fubini's, we thus have that:
    \begin{equation*}
        \mu(\Phi(E)) \ = \ \int_{\Phi(E)}1 \ = \ \int_{E'}\Psi \ = \
        \int_{E'}\int_{0}^{1}\abs{\det(J\Phi)} \ = \ \int_{E}\abs{\det(J\Phi)}
    \end{equation*}
    so the statement holds as needed.
\end{pf}

\newpage
\begin{pf}[title=Composition]
    Suppose the statement holds for $ \Phi:U\gto V $ and $ \Psi:V\gto W $.
    To see that it holds for $ \Psi\circ\Phi $, we have:
    \begin{equation*}
        \mu((\Psi\circ\Phi)(E)) \ = \ \mu(\Psi(\Phi(E))) \ = \
        \int_{\Phi(E)}\abs{\det J\Psi} \ = \ \int_{E}\abs{\det J\Psi}
        \abs{\det J\Phi}
    \end{equation*}
    Note that the equivalence of the determinants follows from chain rule.
\end{pf}

\begin{pf}[title=Locally Simple]
    TBA
\end{pf}

\begin{pf}[title=Finale]
    TBA
\end{pf}

% may exclude based on spacing since its technically big list
% we'll see
\newpage
\begin{thm}[num=10.3,title=Green's Theorem]
    Suppose $ F: M \gto \bR^{2} $ is a smooth vector field on some simple
    region $ M \subseteq \bR^{2} $ with component functions $ P, Q $. Then:
    \begin{equation*}
        \oint_{\p M}P\di x + Q\di y \ = \
        \iint_{M}\frac{\p Q}{\p x}-\frac{\p P}{\p y}
    \end{equation*}
\end{thm}

\begin{pf}
    See Big List 45.
\end{pf}
